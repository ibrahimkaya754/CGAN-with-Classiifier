{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/ibrahimkaya754/GAN/blob/main/dcgan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"e1_Y75QXJS6h"},"source":["### Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dm3J1J7wsKW3"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VZVgc00ds31l"},"outputs":[],"source":["cd drive/MyDrive/ESEN/Scripts/CGAN_with_Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bH7BE3z29NYI"},"outputs":[],"source":["## MODULES AND THE DATA IMPORT\n","from numpy import expand_dims\n","from numpy import zeros\n","from numpy import ones\n","from numpy.random import randn\n","from numpy.random import randint\n","from keras.datasets.fashion_mnist import load_data\n","from tensorflow.keras.optimizers import *\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.initializers import *\n","import tensorflow as tf\n","import numpy as np\n","import pickle\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import MinMaxScaler\n","from numpy import asarray\n","from numpy.random import randn\n","from numpy.random import randint\n","from keras.models import load_model\n","from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.constraints import Constraint\n","from tensorflow.keras.losses import *\n","#from MoGeNA import *\n","from sklearn.preprocessing import *\n","from tensorflow.python.platform import build_info as tf_build_info\n","from scipy import spatial\n","import os\n","print(tf_build_info.build_info)"]},{"cell_type":"code","source":["os.getcwd()"],"metadata":{"id":"DYhH58NLw7Sm"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gez9Mqf4rPys"},"outputs":[],"source":["gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n","for device in gpu_devices:\n","    tf.config.experimental.set_memory_growth(device, True)\n","    print(\"tensorflow version is: \",tf.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V6eiuGf01S4E"},"outputs":[],"source":["# Load The Data\n","def load_wifi_data(dataDict):\n","  for key in dataDict.keys():\n","    print(\"Preparing %s data\" % (key))\n","    data_wifi   = pickle.load(open(dataDict[key][\"datalocation\"],\"rb\"))\n","    labels_wifi = pickle.load(open(dataDict[key][\"labellocation\"],\"rb\"))\n","    \n","    labels  = []\n","    \n","    print(\"data_shape for %s is %s: \" % (key,data_wifi.shape)) \n","    print('label shape for %s is %s: ' % (key,labels_wifi.shape))\n","\n","    for lbl in labels_wifi:\n","        if lbl not in labels:\n","            labels.append(lbl)\n","\n","    print(\"labels: \",labels)\n","    dataDict[key]['data']   = data_wifi\n","    dataDict[key]['labels'] = labels_wifi\n","\n","    dumbarray = np.zeros((dataDict[key]['labels'].shape[0],16),dtype=np.int16)\n","    for ii in range(dataDict[key]['labels'].shape[0]):\n","        indice = labels_wifi[ii]\n","        dumbarray[ii,indice] = 1\n","\n","    dataDict[key]['label1hot']       = dumbarray\n","    dataDict[key]['numberOfClasses'] = len(labels)\n","    print(\"number of classes: \",dataDict[key]['numberOfClasses'])\n","\n","    dataDict[key][\"data\"]= dataDict[key]['data'].reshape(dataDict[key]['data'].shape[0],\n","                                                         dataDict[key]['data'].shape[1],\n","                                                         dataDict[key]['data'].shape[2],1)\n","    print('dataset shape: ', dataDict[key][\"data\"].shape)\n","    print('max value before scaling: ', np.max(dataDict[key][\"data\"]))\n","    print('min value before scaling: ', np.min(dataDict[key][\"data\"]))\n","    print(\"*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--\")\n","  return dataDict"]},{"cell_type":"code","source":["data={\"TrainData\"      :{\"datalocation\"  : \"../newDATA/trainingData_13042022 15:56:46\",\n","                         \"labellocation\" : \"../newDATA/trainingLabel_13042022 15:56:46\"},\n","      \"ValidationData\" :{\"datalocation\"  : \"../newDATA/knowntestData_13042022 15:56:46\",\n","                         \"labellocation\" : \"../newDATA/knowntestLabel_13042022 15:56:46\"},\n","      \"UnknownData\"    :{\"datalocation\"  : \"../newDATA/unknowntestData_18042022 15:36:59\",\n","                         \"labellocation\" : \"../newDATA/unknowntestLabel_18042022 15:36:59\"}}"],"metadata":{"id":"VNgbC0V-yaIL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = load_wifi_data(data)"],"metadata":{"id":"5wIIbRt10uRl"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zyu1i01WrPyy"},"outputs":[],"source":["def scaling(dataDict,scalingA2=\"TrainData\"):\n","\n","  scalerx0 = MinMaxScaler((-0.5,0.5))\n","  scalerx1 = MinMaxScaler((-0.5,0.5))\n","\n","  scx0 = scalerx0.fit(dataDict[scalingA2][\"data\"][:,:,0,0])\n","  scx1 = scalerx1.fit(dataDict[scalingA2][\"data\"][:,:,1,0])\n","\n","  for key in dataDict.keys():\n","    dataDict[key][\"dataScaled\"] = dataDict[key][\"data\"]\n","    dataDict[key][\"dataScaled\"][:,:,0,0]= scx0.transform(dataDict[key][\"data\"][:,:,0,0])\n","    dataDict[key][\"dataScaled\"][:,:,1,0]= scx1.transform(dataDict[key][\"data\"][:,:,1,0])\n","    print('min, max value of I after scaling %s dataset: %.3f, %.3f' % (key,np.min(dataDict[key][\"dataScaled\"][:,:,0,0]),\n","                                                                              np.max(dataDict[key][\"dataScaled\"][:,:,0,0])))\n","    print('min, max value of Q after scaling %s dataset: %.3f, %.3f' % (key,np.min(dataDict[key][\"dataScaled\"][:,:,1,0]),\n","                                                                              np.max(dataDict[key][\"dataScaled\"][:,:,1,0])))\n","\n","  \n","  return dataDict,[scx0,scx1]\n","data,[scx0,scx1] = scaling(data)"]},{"cell_type":"code","source":["def L2normalize(arg):\n","  import keras.backend as K\n","  arg = K.l2_normalize(arg,axis=1)\n","  return arg"],"metadata":{"id":"bZm60SGUqAay"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class model():\n","  def __init__(self, dataDict, AppendOptimizedParams= True,\n","               location= \"../CGAN_with_Classifier_Models/\",\n","               in_shape=(1024,2,1), loadSavedModel= True,\n","               filters=[64,32,16,8], neurons=50, n_classes=16,\n","               strides=[(2,1),(2,2),(2,2),(2,2)], batchSize = 64,\n","               alpha= 0.2, kernelSize= 3, Drop= 0.2, epochs = 50,\n","               learningRate= 0.001, name=\"Module1\",\n","               best_params= [211.0776,207.0222,235.648,\n","                             129.7916,232.7917,0.3253,\n","                             6.730,0.237,0.0002942,1024,5]):\n","    self.in_shape       = in_shape\n","    self.n_classes      = n_classes\n","    self.filters        = filters\n","    self.neurons        = neurons\n","    self.strides        = strides\n","    self.alpha          = alpha\n","    self.kernelSize     = kernelSize\n","    self.Drop           = Drop\n","    self.learningRate   = learningRate\n","    self.modelName      = name\n","    self.data           = dataDict\n","    self.batchSize      = batchSize\n","    self.epochs         = epochs\n","    self.location       = location\n","    self.loadSavedModel = loadSavedModel\n","    self.bestParams     = best_params\n","    self.AppendOptimizedParams = AppendOptimizedParams\n","    self.models         = {}\n","\n","    if self.AppendOptimizedParams:\n","      self.__AppendOptimumPArams()\n","    \n","    self.CallBack()\n","    self.define_classifier()\n","    self.createSiameseModel()\n","\n","  def define_classifier(self):\n","    self.opt = Adam(lr=self.learningRate)\n","    in_image = Input(shape=self.in_shape)\n","    layer1   = Reshape((64,32,1))(in_image)\n","    \n","    for fltr,strd in zip(self.filters,self.strides):\n","        layer1 = Conv2D(int(round(fltr)), (int(round(self.kernelSize)),int(round(self.kernelSize))), \n","                        strides=strd, padding='same')(layer1)\n","        layer1 = LeakyReLU(alpha=self.alpha)(layer1)\n","\n","    flatten                 = Flatten()(layer1)\n","    layer1                  = Dropout(self.Drop)(flatten)\n","    featuredescriptor       = Dense(int(round(self.neurons)))(layer1)\n","    #featuredescriptor      = Lambda(L2normalize)(featuredescriptor)\n","    layer1                  = LeakyReLU(alpha=self.alpha)(featuredescriptor)\n","    layer1                  = Dropout(self.Drop)(layer1)\n","    out                     = Dense(16,activation='softmax')(layer1)\n","\n","    self.classifier  = Model(in_image, out)\n","    \n","    if self.loadSavedModel:\n","      self.classifier.load_weights(self.location+self.modelName+\".hdf5\")\n","    \n","    print(\"NETWORK SUMMARY\")\n","    print(\"--------------------------------------\")\n","    self.classifier.summary()\n","\n","    tf.keras.utils.plot_model(self.classifier,show_shapes=True)\n","\n","    self.siam1 = Model(in_image,featuredescriptor)\n","    self.siam2 = Model(in_image,featuredescriptor)\n","\n","    self.models[self.modelName]= {\"ClassifierModel\": self.classifier,\n","                                  \"Location\"       : self.location+self.modelName+\".hdf5\"}\n","\n","  def trainClassifier(self,isTrain=True,epochs=50,batchSize=1024):\n","    self.epochs    = epochs\n","    self.batchSize = batchSize\n","    self.classifier.compile(loss='categorical_crossentropy', optimizer=self.opt, metrics=['accuracy'])\n","    if isTrain:\n","      self.history = self.models[self.modelName][\"ClassifierModel\"].fit(data['TrainData'][\"dataScaled\"], \n","                                                                        data['TrainData'][\"label1hot\"], \n","                                                                        batch_size=int(round(self.batchSize)), \n","                                                                        epochs=int(round(self.epochs)), \n","                                                                        validation_data=(data['ValidationData'][\"dataScaled\"],\n","                                                                                         data['ValidationData'][\"label1hot\"]), \n","                                                                        verbose=1, shuffle=True,\n","                                                                        callbacks=[self.callbacks[\"checkpoint\"], \n","                                                                                   self.callbacks[\"reduce_lr\"]])\n","      self.models[self.modelName][\"modelhistory\"] = self.history \n","                                    \n","  def CallBack(self):\n","    self.callbacks = {}\n","    self.callbacks[\"checkpoint\"]     =  ModelCheckpoint(filepath=self.location+self.modelName+\".hdf5\", \n","                                        monitor='val_accuracy', verbose=1, \n","                                        save_best_only=True, period=1, \n","                                        mode='max',save_weights_only=False)\n","    self.callbacks[\"reduce_lr\"]      =  ReduceLROnPlateau(monitor='val_accuracy', \n","                                        factor=0.98,\n","                                        patience=10, \n","                                        min_lr=0.0000001, mode='max', verbose=1)\n","    self.callbacks[\"early_stopping\"] =  EarlyStopping(monitor='val_accuracy', \n","                                                      patience=250, verbose=1, \n","                                                      mode='auto')\n","  \n","  ## __AppendOptimumPArams appends the optimum parameters obtained in GA to network                                                     \n","  def __AppendOptimumPArams(self):\n","    ## This function appends the optimum values \n","    ## obtained from GA to the classifier model.\n","    self.filters      = self.bestParams[0:4]\n","    self.neurons      = self.bestParams[4]\n","    self.alpha        = self.bestParams[5]\n","    self.kernelSize   = self.bestParams[6]\n","    self.Drop         = self.bestParams[7]\n","    self.learningRate = self.bestParams[8]\n","    self.batchSize    = int(round(self.bestParams[9]))\n","    self.epochs       = int(round(self.bestParams[10]))\n","    print(\"\\nOPTIMUM PARAMETER SET APPENDED\")\n","    print(\"--------------------------------------\")\n"," \n","  ## __distance just calculates the element-wise absolute difference of two tensors\n","  def __distance(self,args):\n","    tensor1,tensor2 = args\n","    dist = tf.abs((tensor1 - tensor2))\n","    return dist\n","  \n","  def createSiameseModel(self):\n","    inp1     = Input(shape=(1024,2,1),name=\"Siaminput1\")\n","    inp2     = Input(shape=(1024,2,1),name=\"Siaminput2\")\n","    d1out    = self.siam1(inp1)\n","    d2out    = self.siam2(inp2)\n","    l1dist   = Lambda(self.__distance)([d1out,d2out])\n","    l1dist   = Lambda(L2normalize)(l1dist)\n","    out      = Dense(1,activation=\"sigmoid\")(l1dist)\n","    self.Siam = Model([inp1,inp2],out)\n","    self.Siam.compile(loss='binary_crossentropy', optimizer=self.opt, metrics=['accuracy'])\n","    \n","    print(\"\\n\\nSIAM NETWORK SUMMARY\")\n","    print(\"--------------------------------------\")\n","    self.Siam.summary()\n","    self.models[self.modelName][\"SiamModel\"] = self.Siam\n","  \n","  ##Â __createSiamDataCouples creates data couples for siamese network training\n","  def __createSiamDataCouples(self,batchSize,dataset):\n","    labels_siamese = list(np.ones(batchSize))\n","    siamese_inp1   = []\n","    siamese_inp2   = []\n","    indiceCouples  = np.random.randint(0,len(dataset[\"dataScaled\"]),(batchSize,2))\n","    for ii in range(batchSize):\n","      indice1 = indiceCouples[ii,0]\n","      indice2 = indiceCouples[ii,1]\n","      siamese_inp1.append(dataset[\"dataScaled\"][indice1])\n","      siamese_inp2.append(dataset[\"dataScaled\"][indice2])\n","      if dataset[\"labels\"][indice1]==dataset[\"labels\"][indice2]:\n","        labels_siamese[ii] = 0.0\n","    return np.array(siamese_inp1),np.array(siamese_inp2),np.array(labels_siamese)\n","\n","  def trainSiamese(self, Nepochs=100, batchSize=512,\n","                   trainFeatureExtractors= False, \n","                   load_saved_data=False):   \n","      \n","    batchPerEp    = int(self.data[\"TrainData\"][\"dataScaled\"].shape[0] / batchSize)\n","\n","    if load_saved_data:\n","      self.siam1.load_weights(self.location+self.modelName+'_siam1.hdf5')\n","      self.siam2.load_weights(self.location+self.modelName+'_siam2.hdf5')\n","      self.Siam.load_weights(self.location+self.modelName+'_siamModelUnited.hdf5')\n","\n","    # We make the extracted features from classifier non-trainable here.\n","    if trainFeatureExtractors:\n","      self.siam1.trainable = True\n","      self.siam2.trainable = True\n","    else:\n","      self.siam1.trainable = False\n","      self.siam2.trainable = False\n","\n","    # manually enumerate epochs\n","    for i in range(Nepochs):\n","      if i%25 == 0:\n","        # save the generator model\n","        self.siam1.save(self.location+self.modelName+'_siam1.hdf5')\n","        self.siam2.save(self.location+self.modelName+'_siam2.hdf5')\n","        self.Siam.save(self.location+self.modelName+'_siamModelUnited.hdf5')\n","      # enumerate batches over the training set\n","      for j in range(batchPerEp):\n","        # get randomly selected samples, train and calculate loss\n","        siamese_inp1,siamese_inp2,labels_siamese = self.__createSiamDataCouples(batchSize, self.data[\"TrainData\"])\n","        loss, _ = self.Siam.train_on_batch([siamese_inp1,siamese_inp2], labels_siamese)\n","        \n","        # calculate validation loss\n","        siamese_inp1_val,siamese_inp2_val,labels_siamese_val = self.__createSiamDataCouples(batchSize, self.data[\"ValidationData\"])\n","        outval   = np.squeeze(self.Siam([siamese_inp1_val,siamese_inp2_val]))\n","        bce      = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","        val_loss = bce(labels_siamese_val, outval).numpy()\n","\n","        print('epoch: %d/%d, %d/%d, loss: %.3f, valloss: %.3f' % (i+1, Nepochs, j+1, batchPerEp, loss, val_loss))\n","      print(\"-----------------------------------------------\")\n","\n","    # save the generator model\n","    self.siam1.save(self.location+self.modelName+'_siam1.hdf5')\n","    self.siam2.save(self.location+self.modelName+'_siam2.hdf5')\n","    self.Siam.save(self.location+self.modelName+'_siamModelUnited.hdf5')\n"],"metadata":{"id":"1xYJchqHiwcA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mymodel = model(data,name=\"Module1\", loadSavedModel=True, AppendOptimizedParams=True)"],"metadata":{"id":"hkeLjGP3iwzr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mymodel.data[\"TrainData\"].keys()"],"metadata":{"id":"zD1CRbWiSYaZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mymodel.__dict__.keys()"],"metadata":{"id":"GTexw4QYiw7d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mymodel.trainClassifier(isTrain=True,epochs=1)"],"metadata":{"id":"2KWgr7oV3snU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mymodel.trainSiamese(Nepochs=10, batchSize=512,\n","                     trainFeatureExtractors= False, \n","                     load_saved_data=False)"],"metadata":{"id":"jlIV-URcrEuR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mymodel.models[\"Module1\"]"],"metadata":{"id":"JgQ7ZJjtpIPQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Below is the section to find Cosine Similarity (CS) of the Classes.\n","\n","The CS is calculated for every class pairs, e.g. 0-0, 0,1, 0,2, etc.\n","\n","The dictionary \"dict_mean_cos\" is the dictionary keeping all the values for CS."],"metadata":{"id":"NUqCVIqeeJF9"}},{"cell_type":"code","source":["## We define CS4OriginalData function to make the calculation for CS for all class pairs.\n","## The dictionary \"dict_mean_cos\" is created inside this function.\n","def CS4OriginalData(NumberOfDataPickedRandomly,model,dataset1,dataset2):\n","  siam1            = model\n","  siam2            = modelDict[\"siam2\"] \n","  siamUnited       = modelDict[\"siamModelUnited\"] \n","  nOfClass1        = dataset1[2].shape[1]\n","  dict_mean_cos1   = {str(ii): {\"indices\":[]} for ii in range(nOfClass1)}\n","  nOfClass2        = dataset2[2].shape[1]\n","  dict_mean_cos2   = {str(ii): {\"indices\":[]} for ii in range(nOfClass2)}\n","  total_DataNumber1= 0\n","  total_DataNumber2= 0\n","\n","  for key in dict_mean_cos1.keys():\n","    for ii in range(len(dataset1[1])):\n","      if dataset1[1][ii] == int(key):\n","        dict_mean_cos1[key][\"indices\"].append(ii)\n","\n","  for key in dict_mean_cos2.keys():\n","    for ii in range(len(dataset2[1])):\n","      if dataset2[1][ii] == int(key):\n","        dict_mean_cos2[key][\"indices\"].append(ii)\n","\n","\n","  for key in dict_mean_cos1.keys():\n","    dict_mean_cos1[key][\"shape\"] = dataset1[1][dict_mean_cos1[key][\"indices\"]].shape[0]\n","    print(\"number of data existing for each class %s: %d for dataset1\" % (key,dict_mean_cos1[key][\"shape\"]))\n","    total_DataNumber1 = total_DataNumber1 + dict_mean_cos1[key][\"shape\"]\n","  \n","  print(\"----------------------------------------------------------------------------------------\\n\")\n","  \n","  for key in dict_mean_cos2.keys():\n","    dict_mean_cos2[key][\"shape\"] = dataset2[1][dict_mean_cos2[key][\"indices\"]].shape[0]\n","    print(\"number of data existing for each class %s: %d for dataset2\" % (key,dict_mean_cos2[key][\"shape\"]))\n","    total_DataNumber2 = total_DataNumber2 + dict_mean_cos2[key][\"shape\"]\n","\n","\n","  print(\"----------------------------------------------------------------------------------------\\n\")\n","  print(\"total_data_number for dataset1: %d\" % (total_DataNumber1))\n","  print(\"dataset1 shape is: \", dataset1[1].shape[0])\n","  print(\"----------------------------------------------------------------------------------------\")\n","  print(\"total_data_number for dataset2: %d\" % (total_DataNumber2))\n","  print(\"dataset2 shape is: \", dataset2[1].shape[0])\n","  print(\"----------------------------------------------------------------------------------------\\n\")\n","  for key in dict_mean_cos1.keys():\n","    try:\n","      dict_mean_cos1[key][\"siam1_Predictions\"] = siam1.predict(dataset1[0][dict_mean_cos1[key][\"indices\"]])\n","    except:\n","      continue\n","  \n","  for key in dict_mean_cos2.keys():\n","    try:\n","      dict_mean_cos2[key][\"siam2_Predictions\"] = siam2.predict(dataset2[0][dict_mean_cos2[key][\"indices\"]])\n","    except:\n","      continue\n","\n","  for key1 in dict_mean_cos1.keys():\n","    len1 = dict_mean_cos1[key1][\"shape\"]\n","    for key2 in dict_mean_cos2.keys():\n","      len2 = dict_mean_cos2[key2][\"shape\"]   \n","      \n","      try:\n","        list_cs   = []\n","        list_Siam = []\n","        lenmin    = np.minimum(len1,len2)\n","        \n","        for ii in range(NumberOfDataPickedRandomly):\n","          ind1, ind2 = np.random.randint(0,lenmin,2)\n","          ##### HERE IS FOR COSINE SIMILARITY ##########\n","          cosinesimilarity = 1 - spatial.distance.cosine(dict_mean_cos1[key1][\"siam1_Predictions\"][ind1], \n","                                                         dict_mean_cos2[key2][\"siam2_Predictions\"][ind2])\n","          list_cs.append(cosinesimilarity)\n","\n","          ##### HERE IS FOR SIAMESE SIMILARITY ##########\n","          siameseResult = siamUnited([dataset1[0][dict_mean_cos1[key1][\"indices\"][ind1]].reshape(1,1024,2,1),\n","                                      dataset2[0][dict_mean_cos2[key2][\"indices\"][ind2]].reshape(1,1024,2,1)])\n","          list_Siam.append(1 - siameseResult)\n","        \n","        string1 = \"mean cosine similarity \"\n","        string2 = \"mean siamese result\"\n","        print(\"%s is: %.3f and %s is: %.3f for %s and %s\" % (string1,np.mean(list_cs),\n","                                                             string2,np.mean(list_Siam),\n","                                                             key1,key2))\n","      except:\n","        continue\n","    \n","    print(\"---------------------------------------------------------------------------------------\")\n","  return dict_mean_cos1,dict_mean_cos2"],"metadata":{"id":"coLzQm_-iTzS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def OptimumDataPickNumber(list_number=[50]):\n","  dict_result_OptimizedClassifier1 = {}\n","  dict_result_OptimizedClassifier2 = {}\n","  for RandomDataNo in list_number:\n","    print(\"RUNNING FOR OptimizedClassifier1\")\n","    _,_ = CS4OriginalData(RandomDataNo,OptimizedClassifier1,dataset,dataset)\n","    print(\"RUNNING FOR OptimizedClassifier2\")\n","    _,_ = CS4OriginalData(RandomDataNo,OptimizedClassifier2,dataset,dataset)\n","  return dict_result_OptimizedClassifier1, dict_result_OptimizedClassifier2\n","  \n","_,_ = OptimumDataPickNumber()"],"metadata":{"id":"3RvFEFvHiUGC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Here We write the results to pickle.\n","#unitedResult= [dict_result_OptimizedClassifier1, dict_result_OptimizedClassifier2]\n","#pickle.dump(unitedResult,open(\"dict_result_opt.pickle\",\"wb\"))"],"metadata":{"id":"eVnokZ-RuQ0H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def OptimumDataPickNumber(list_number=[50]):\n","  dict_result_OptimizedClassifier1 = {}\n","  dict_result_OptimizedClassifier2 = {}\n","  for RandomDataNo in list_number:\n","    print(\"RUNNING FOR OptimizedClassifier1\")\n","    _,_ = CS4OriginalData(RandomDataNo,OptimizedClassifier1,dataset,datasetVal)\n","    print(\"RUNNING FOR OptimizedClassifier2\")\n","    _,_ = CS4OriginalData(RandomDataNo,OptimizedClassifier2,dataset,datasetVal)\n","  return dict_result_OptimizedClassifier1, dict_result_OptimizedClassifier2\n","  \n","_,_ = OptimumDataPickNumber()"],"metadata":{"id":"WnbB0oc0lLNj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def OptimumDataPickNumber(list_number=[50]):\n","  dict_result_OptimizedClassifier1 = {}\n","  dict_result_OptimizedClassifier2 = {}\n","  for RandomDataNo in list_number:\n","    print(\"RUNNING FOR OptimizedClassifier1\")\n","    _,_ = CS4OriginalData(RandomDataNo,OptimizedClassifier1,dataset,datasetVal_unknown)\n","    print(\"RUNNING FOR OptimizedClassifier2\")\n","    _,_ = CS4OriginalData(RandomDataNo,OptimizedClassifier2,dataset,datasetVal_unknown)\n","  return dict_result_OptimizedClassifier1, dict_result_OptimizedClassifier2\n","  \n","_,_ = OptimumDataPickNumber()"],"metadata":{"id":"ZHNWfymPlLkB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"TVCZ-RuzlLmL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## BELOW IS THE PART WHERE WE RUN GENETIC ALGORITM OPTIMIZER"],"metadata":{"id":"Sbqbi_KkAxRD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"asQqcGO3rPyz"},"outputs":[],"source":["# Function for MoGenA\n","def optClassifier(inp):\n","    print(\"parameter set is: \",inp)\n","    filters      = inp[0:4]\n","    neurons      = inp[4]\n","    alpha        = inp[5]\n","    kernelSize   = inp[6]\n","    Drop         = inp[7]\n","    learningRate = inp[8]\n","    classifier   = define_classifier(in_shape=(9,512,1), n_classes=14, \n","                                     filters=filters, neurons=neurons,\n","                                     strides=[(2,1),(2,2),(2,2),(2,2)],\n","                                     alpha= alpha, kernelSize= kernelSize, Drop= Drop,\n","                                     learningRate= learningRate)\n","    history      = classifier.fit(dataset[0], dataset[2], \n","                                  batch_size=int(round(inp[9])), \n","                                  epochs=int(round(inp[10])), \n","                                  validation_data=(datasetVal[0],datasetVal[2]), \n","                                  #validation_split = 0.25,\n","                                  verbose=1, shuffle=True,\n","                                  callbacks=[checkpoint, reduce_lr])\n","    minimization_param = -np.max(history.history['val_accuracy'])\n","    return minimization_param"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5H8FKK1RrPyz","scrolled":true},"outputs":[],"source":["# Genetic Algorithm\n","runOpt = False\n","if runOpt:\n","  ga = Mogena.MoGenA(fitness_func         = optClassifier, \n","                    number_of_generation = 20,\n","                    number_of_variables  = 11,\n","                    population_size      = 30,\n","                    lower_boundaries     = [8  ,8  ,8  ,8  ,8  ,0.1,3,0.1,0.00001,64  ,20 ],\n","                    upper_boundaries     = [250,250,250,250,250,0.5,7,0.5,0.001  ,1024,200],\n","                    use_saved_data       = False)\n","  result =  {'best_fitness_ever'           : ga.best_fitness_ever,\n","            'best_individual_decoded_ever' : ga.best_individual_decoded_ever}"]},{"cell_type":"code","source":[""],"metadata":{"id":"OMjIGA6YlLoj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"DC9E3ZukiUKJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"KXPSzRbAiUNi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Below is the section where we load the synthetically generated data and compare it with the original one using cosine similarity."],"metadata":{"id":"bu6Yzw9dfcIf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jw-e7ih6xzcA"},"outputs":[],"source":["def load_synthetic_data(data_location,classifier=OptimizedClassifier[\"classifier\"]):\n","  synthetic_data   = pickle.load(open(data_location,\"rb\"))\n","  synthetic_data[:,:,0]= scx0.transform(synthetic_data[:,:,0])\n","  synthetic_data[:,:,1]= scx1.transform(synthetic_data[:,:,1])\n","  print(\"Synthetic Data Shape is: \", synthetic_data.shape)\n","  synthetic_data_result = classifier.predict(synthetic_data)\n","  listClassifiedSyntheticData = [np.argmax(synthetic_data_result[ii]) for ii in range(len(synthetic_data))]\n","  print(\"Generated Classes are: \", listClassifiedSyntheticData)\n","  return synthetic_data, listClassifiedSyntheticData\n","synthetic_data, listClassifiedSyntheticData= load_synthetic_data(\"../mygenerateddata_normal2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XJ07D_Cu0Yoo"},"outputs":[],"source":["## CompareCS4SynData is to compare the cosine similarity for the synthetic (syn) data and the original data.\n","## We compare the synthetic data classes determined by the classifier with the original classes existing in the original dataset\n","def CompareCS4SynData(synthetic_data,listClassifiedSyntheticData,siam1,siam2,dict_mean_cos):\n","  res2 = siam2.predict(synthetic_data)\n","  for key,value in enumerate(listClassifiedSyntheticData):\n","    res1 = siam1.predict(dataset[0][dict_mean_cos[str(value)][\"indices\"]])\n","    list_cs = []\n","    for ii in range(100):\n","      ind1 = np.random.randint(0,2500,1)\n","      cosinesimilarity = 1- spatial.distance.cosine(res1[ind1], res2[key])\n","      #print(cosinesimilarity)\n","      list_cs.append(cosinesimilarity)\n","    string = \"mean cosine similarity for class %s and 'newly generated class %s'\" % (value,value)\n","    print(\"%s is: %.3f\" % (string,np.mean(list_cs)))\n","\n","CompareCS4SynData(synthetic_data,listClassifiedSyntheticData,\n","                  OptimizedClassifier[\"siam1\"],OptimizedClassifier[\"siam2\"],\n","                  dict_mean_cos=dict_result_opt[\"50\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G2tj49Kk0Yqo"},"outputs":[],"source":["plt.scatter(listClassifiedSyntheticData,np.arange(len(listClassifiedSyntheticData)))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6YmXmyZGBFAq"},"outputs":[],"source":["# Postprocess\n","def postprocess(model,dataset):\n","    OptimizedClassifier[\"classifier\"].load_weights(\"./classifier.hdf5\")\n","    results = model.predict(dataset[0])\n","    list_class = []\n","    for ii in range(results.shape[0]):\n","        list_class.append(np.argmax(results[ii]))\n","\n","    counterTrue  = 0\n","    counterFalse = 0\n","    for ii in range(len(list_class)):\n","        if list_class[ii] == dataset[1][ii]:\n","            counterTrue = counterTrue + 1\n","        else:\n","            counterFalse = counterFalse + 1\n","    print(\"True:%d    False:%d\" % (counterTrue,counterFalse))\n","    return list_class,counterTrue,counterFalse"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P8u-DKqKGz0E"},"outputs":[],"source":["list_class,counterTrue,counterFalse=postprocess(OptimizedClassifier[\"classifier\"],datasetVal)"]},{"cell_type":"markdown","source":["## GENERATIVE MODEL (CONDITIONAL GAN)"],"metadata":{"id":"GkFdHFmnA_dB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nkMeyqNcAYAs"},"outputs":[],"source":["class Spectral_Norm(Constraint):\n","    def __init__(self, power_iters=5):\n","        self.n_iters = power_iters\n","        \n","    def l2_normalize(self, x, eps=1e-12):\n","        return x / tf.linalg.norm(x + eps)\n","\n","    def __call__(self, w):\n","        flattened_w = tf.reshape(w, [w.shape[0], -1])\n","        u = tf.random.normal([flattened_w.shape[0]])\n","        v = tf.random.normal([flattened_w.shape[1]])\n","        for i in range(self.n_iters):\n","            v = tf.linalg.matvec(tf.transpose(flattened_w), u)\n","            v = self.l2_normalize(v)\n","            u = tf.linalg.matvec(flattened_w, v)\n","            u = self.l2_normalize(u)\n","        sigma = tf.tensordot(u, tf.linalg.matvec(flattened_w, v), axes=1)\n","        return w / sigma\n","\n","    def get_config(self):\n","        return {'n_iters': self.n_iters}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VFnJtELM9nna"},"outputs":[],"source":["# define the standalone discriminator model\n","def define_discriminator(in_shape=(1024,2,1), n_classes=numberOfClasses, convDim='2D'):\n","    # label input\n","    initializer = tf.keras.initializers.GlorotNormal()\n","    const = Spectral_Norm()\n","    in_label = Input(shape=(1,16))\n","    # embedding for categorical input\n","    li = Embedding(16, 50)(in_label)\n","    # scale up to image dimensions with linear activation\n","    n_nodes = in_shape[0] * in_shape[1]\n","    li = Dense(n_nodes)(li)\n","    if convDim == '1D':\n","        # reshape to additional channel\n","        li = Reshape((in_shape[0], in_shape[1]))(li)\n","        # image input\n","        in_image = Input(shape=in_shape[0:2])\n","        # concat label as a channel\n","        merge = Concatenate()([in_image, li])\n","        merge = Reshape((64,64))(merge)\n","        # downsample\n","        layer1 = Conv1D(128, (3), strides=(2), padding='same',\n","                        kernel_initializer = glorot_normal(),\n","                        kernel_constraint= const)(merge)\n","        layer1 = LeakyReLU(alpha=0.2)(layer1)\n","\n","        layer1 = Conv1D(64, (3), strides=(2), padding='same',\n","                        kernel_initializer = glorot_normal(),\n","                        kernel_constraint= const)(layer1)\n","        layer1 = LeakyReLU(alpha=0.2)(layer1)\n","\n","        layer1 = Conv1D(32, (3), strides=(2), padding='same',\n","                        kernel_initializer = glorot_normal(),\n","                        kernel_constraint= const)(layer1)\n","        layer1 = LeakyReLU(alpha=0.2)(layer1)\n","\n","    else:\n","        leakyReluAlpha = 0.2\n","        ksize = (3,3)\n","        # reshape to additional channel\n","        li = Reshape((in_shape[0], in_shape[1], 16))(li)\n","        # image input\n","        in_image = Input(shape=in_shape)\n","        # concat label as a channel\n","        merge = Concatenate()([in_image, li])\n","        merge = Reshape((64,32,17))(merge)\n","        # downsample\n","        layer1 = Conv2D(128, ksize, strides=(2,1), padding='same',\n","                        kernel_initializer = initializer,\n","                        kernel_constraint= const)(merge)\n","        layer1 = LeakyReLU(alpha=leakyReluAlpha)(layer1)\n","\n","        layer1 = Conv2D(64, ksize, strides=(2,2), padding='same',\n","                        kernel_initializer = initializer,\n","                        kernel_constraint= const)(layer1)\n","        layer1 = LeakyReLU(alpha=leakyReluAlpha)(layer1)\n","\n","        layer1 = Conv2D(32, ksize, strides=(2,2), padding='same',\n","                        kernel_initializer = initializer,\n","                        kernel_constraint= const)(layer1)\n","        layer1 = LeakyReLU(alpha=leakyReluAlpha)(layer1)\n","        \n","        layer1 = Conv2D(16, ksize, strides=(2,2), padding='same',\n","                        kernel_initializer = initializer,\n","                        kernel_constraint= const)(layer1)\n","        layer1 = LeakyReLU(alpha=leakyReluAlpha)(layer1)\n","\n","    layer1 = Flatten()(layer1)\n","    layer1 = Dropout(0.2)(layer1)\n","    # output\n","    out_layer = Dense(1, activation='sigmoid')(layer1)\n","    # define model\n","    model = Model([in_image, in_label], out_layer)\n","    # compile model\n","    opt = RMSprop(lr=0.0001)\n","    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","    model.summary()\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gcZKpierWzHk"},"outputs":[],"source":["# size of the latent space\n","latent_dim = 100"]},{"cell_type":"code","source":["numberOfClasses"],"metadata":{"id":"UQhfC7Rb-q24"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"orYWSZLT9toc"},"outputs":[],"source":["# define the standalone generator model\n","def define_generator(latent_dim, n_classes=numberOfClasses, convDim='2D'):\n","    # label input\n","    initializer = tf.keras.initializers.GlorotNormal()\n","    in_label = Input(shape=(1,16))\n","    # embedding for categorical input\n","    li = Embedding(16, 50)(in_label)\n","    # linear multiplication\n","  \n","    if convDim== '1D':\n","        n_nodes = 8 * 16 \n","        li = Dense(n_nodes)(li)\n","        # reshape to additional channel\n","        li = Reshape((64, 2))(li)\n","        # image generator input\n","        in_lat = Input(shape=(latent_dim,))\n","        # foundation for 7x7 image\n","        n_nodes = 64 * 64\n","        gen = Dense(n_nodes)(in_lat)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","        gen = Reshape((64, 64))(gen)\n","        # merge image gen and label input\n","        merge = Concatenate()([gen, li])\n","          # upsample to 14x14\n","        gen = Conv1DTranspose(16, (4), strides=(4), padding='same',kernel_initializer = glorot_normal())(merge)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","        # upsample to 28x28\n","        gen = Conv1DTranspose(16, (4), strides=(4), padding='same',kernel_initializer = glorot_normal())(gen)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","\n","        gen = Conv1DTranspose(16, (4), strides=(2), padding='same',kernel_initializer = glorot_normal())(gen)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","        # output\n","        gen = Conv1D(1, (4), activation='tanh', padding='same',kernel_initializer = glorot_normal())(gen)\n","        out_layer = Reshape((1024, 2))(gen)\n","    else:\n","        n_nodes = 4 * 4\n","        li = Dense(n_nodes)(li)\n","        # reshape to additional channel\n","        li = Reshape((16, 4*4, 1))(li)\n","        # image generator input\n","        in_lat = Input(shape=(latent_dim,))\n","        # foundation for 7x7 image\n","        n_nodes = 4 * 4 * 16\n","        gen = Dense(n_nodes)(in_lat)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","        gen = Reshape((16, 4*4, 1))(gen)\n","        # merge image gen and label input\n","        merge = Concatenate()([gen, li])\n","\n","        gen = Conv2DTranspose(256, (4,4), strides=(2,2), padding='same',kernel_initializer = initializer)(merge)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","\n","        gen = Conv2DTranspose(128, (4,4), strides=(2,1), padding='same',kernel_initializer = initializer)(gen)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","\n","        gen = Conv2DTranspose(128, (4,4), strides=(1,1), padding='same',kernel_initializer = initializer)(gen)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","\n","        gen = Conv2DTranspose(64, (4,4), strides=(1,1), padding='same',kernel_initializer = initializer)(gen)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","    \n","        gen = Conv2D(1, (3,3), strides=(1,1), activation='tanh', padding='same',\n","                     kernel_initializer = initializer)(gen)\n","        gen = Dropout(0.2)(gen)\n","    out_layer = Reshape((1024, 2, 1))(gen)\n","    # define model\n","    model = Model([in_lat, in_label], out_layer)\n","    model.summary()\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UD1E8McUoppi"},"outputs":[],"source":["# Inception Score for measuring the similarity between the generated signals and the real ones, using KL Divergence\n","def InceptionScore(nelem=100):\n","    for tt in range(1):\n","        RealImageList = []\n","        latent_points, labels = generate_latent_points(latent_dim=100, n_samples=30)\n","        for elem in range(labels.shape[0]):\n","            nolabel = np.argmax(labels[elem])\n","            for ii in range(1000):\n","                if dataset[1][ii] == nolabel:\n","                    indice = ii\n","        \n","            RealImageList.append(dataset[0][indice])\n","\n","        RealImageList= np.array(RealImageList)\n","        X1  = g_model.predict([latent_points, labels])\n","        kl  = KLDivergence()\n","        bce = BinaryCrossentropy(from_logits=True)\n","\n","    return kl(RealImageList,X1).numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v6oBxiZS9wrY"},"outputs":[],"source":["# define the combined generator and discriminator model, for updating the generator\n","def define_gan(g_model, d_model):\n","    # make weights in the discriminator not trainable\n","    #d_model.trainable = False\n","\n","    in_label = Input(shape=(1,16))\n","    in_lat = Input(shape=(latent_dim,))\n","\n","    gen_out = g_model([in_lat,in_label])\n","\n","    gan_out = d_model([gen_out,in_label])\n","\n","    # get noise and label inputs from generator model\n","    #gen_noise, gen_label = g_model.input\n","    # get image output from the generator model\n","    #gen_output = g_model.output\n","    # connect image output and label input from generator as inputs to discriminator\n","    #gan_output = d_model([gen_output, gen_label])\n","\n","    # define gan model as taking noise and label and outputting a classification\n","    model = Model([in_lat,in_label], gan_out)\n","    # compile model\n","    opt = RMSprop(lr=0.0003)\n","    model.compile(loss='binary_crossentropy', optimizer=opt)\n","    #model.compile(loss=Ganloss, optimizer=opt)\n","    model.summary()\n","\n","    return model"]},{"cell_type":"code","source":["dataset[2].shape"],"metadata":{"id":"O9RgHPpo-3NT"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rv02rxjoXSCa"},"outputs":[],"source":["# select real samples\n","def generate_real_samples(dataset, n_samples):\n","\t# split into images and labels\n","\timages, labels, onehotenc = dataset\n","\t# choose random instances\n","\tix = randint(0, images.shape[0], n_samples)\n","\t# select images and labels\n","\tX, labels, onehotenc = images[ix], labels[ix], onehotenc[ix]\n","\tonehotenc = onehotenc.reshape((n_samples,1,16))\n","\t# generate class labels\n","\ty = np.ones((n_samples, 1))\n","\treturn [X, onehotenc], [y, onehotenc]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"trF69d8AXWDi"},"outputs":[],"source":["# generate points in latent space as input for the generator\n","def generate_latent_points(latent_dim, n_samples, n_classes=numberOfClasses):\n","\t# generate points in the latent space\n","\tx_input = randn(latent_dim * n_samples)\n","\t# reshape into a batch of inputs for the network\n","\tz_input = x_input.reshape(n_samples, latent_dim)\n","\t# generate labels\n","\tlabels = randint(0, n_classes, n_samples)\n","\tdumbarray = np.zeros((n_samples,1,16))\n","\tfor ii in range(n_samples):\n","\t\tindice = labels[ii]\n","\t\tdumbarray[ii,0,indice] = 1\n","\treturn [z_input, dumbarray]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kUKaHkbvXZrr"},"outputs":[],"source":["# use the generator to generate n fake examples, with class labels\n","def generate_fake_samples(generator, z_input, labels_input, n_samples):\n","\t# predict outputs\n","\timages = generator.predict([z_input, labels_input])\n","\t# create class labels\n","\ty = np.zeros((n_samples, 1))\n","\treturn [images, labels_input], y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x3ATFLSGrPy-","scrolled":true},"outputs":[],"source":["# create the discriminator\n","d_model = define_discriminator(convDim='2D')\n","tf.keras.utils.plot_model(d_model, show_shapes=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZsL6GlPuV_Dw"},"outputs":[],"source":["# create the generator\n","g_model = define_generator(latent_dim,convDim='2D')\n","tf.keras.utils.plot_model(g_model, show_shapes=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RCofZKnlXHI_","scrolled":true},"outputs":[],"source":["# create the gan\n","gan_model = define_gan(g_model, d_model)\n","tf.keras.utils.plot_model(gan_model,show_shapes=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"12Em1-YedsmR"},"outputs":[],"source":["# train the generator and discriminator\n","def train(g_model, d_model, gan_model, dataset, latent_dim, \n","          n_epochs=100, n_batch=128, load_saved_data=True, \n","          dclip_value=0.1, gclip_value=0.1):\n","    \n","    if load_saved_data:\n","        g_model.load_weights('cgan_generator.hdf5')\n","        d_model.load_weights('cgan_discriminator.hdf5')\n","  \n","    bat_per_epo = int(dataset[0].shape[0] / n_batch)\n","    half_batch = int(n_batch / 2)\n","    # manually enumerate epochs\n","    for i in range(n_epochs):\n","        if i%10 == 0:\n","          # save the generator model\n","            g_model.save('cgan_generator.hdf5')\n","            d_model.save('cgan_discriminator.hdf5')\n","        # enumerate batches over the training set\n","        for j in range(bat_per_epo):\n","            list_z_input = []\n","            list_labels_input =[]\n","            d_model.trainable = True\n","            for ii in range(5):\n","                # get randomly selected 'real' samples\n","                [X_real, labels_real], [y_real, onehotenc] = generate_real_samples(dataset, n_batch)\n","                # update discriminator model weights\n","                d_loss1, _ = d_model.train_on_batch([X_real, labels_real], y_real)\n","                # generate 'fake' examples\n","                # prepare points in latent space as input for the generator\n","                [z_input, labels_input]  = generate_latent_points(latent_dim, n_batch)\n","                [X_fake, labels], y_fake = generate_fake_samples(g_model, z_input, labels_input, n_batch)\n","                # update discriminator model weights\n","                d_loss2, _ = d_model.train_on_batch([X_fake, labels], y_fake)\n","\n","                list_z_input.append(z_input)\n","                list_labels_input.append(labels_input)\n","\n","                for l in d_model.layers:\n","                    weights = l.get_weights()\n","                    weights = [np.clip(w, -dclip_value, dclip_value) for w in weights]\n","                    l.set_weights(weights)\n","\n","            zinput = np.ndarray((5*n_batch, latent_dim))\n","            labelsinput = np.ndarray((5*n_batch,1,16))\n","            cntr = 0\n","            for ii in range(5):\n","                for jj in range(n_batch):\n","                    zinput[cntr,:] = list_z_input[ii][jj,:]\n","                    labelsinput[ii*n_batch:(ii+1)*n_batch,:] = list_labels_input[ii][:]\n","                    cntr = cntr + 1\n","\n","            # create inverted labels for the fake samples\n","            y_gan = np.ones((5*n_batch, 1))\n","            # update the generator via the discriminator's error\n","            d_model.trainable = False\n","            #g_loss = gan_model.train_on_batch([z_input, labels_input], y_gan)\n","            g_loss = gan_model.train_on_batch([zinput, labelsinput], y_gan)\n","\n","            for l in g_model.layers:\n","                weights = l.get_weights()\n","                weights = [np.clip(w, -gclip_value, gclip_value) for w in weights]\n","                l.set_weights(weights)\n","            # summarize loss on this batch\n","            try:\n","                inceptionScore = InceptionScore(nelem=n_batch)\n","            except:\n","                continue\n","            print('epoch: %d/%d, %d/%d, dloss_real: %.3f, dloss_fake: %.3f, gloss: %.3f, IS: %s' %\\\n","                  (i+1, n_epochs, j+1, bat_per_epo, d_loss1, d_loss2, g_loss, inceptionScore))\n","\n","        ix = np.random.randint(0,half_batch)\n","        #print('generated label : ', labels)\n","        #print(\"***************************\")\n","        #print('real label      : ', labels_real)\n","        X_real_abs = X_real[ix,:,0] + 1j*X_real[ix,:,1]\n","        X_fake_abs = X_fake[ix,:,0] + 1j*X_fake[ix,:,1]\n","\n","        print('\\n')\n","        plt.figure(figsize=(15,6))\n","        plt.plot(np.abs(X_real_abs))\n","        plt.plot(np.abs(X_fake_abs))\n","        plt.show()\n","        print('\\n')\n","\n","    # save the generator model\n","    g_model.save('cgan_generator.hdf5')\n","    d_model.save('cgan_discriminator.hdf5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jN3kHdLXXfCj","scrolled":true},"outputs":[],"source":["# train model\n","train(g_model, d_model, gan_model, dataset, latent_dim, \n","      n_epochs=100, n_batch=256, load_saved_data=True, \n","      dclip_value=0.1, gclip_value=0.5)    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"luQfSTz5iXpr"},"outputs":[],"source":["g_model.load_weights('cgan_generator.hdf5')\n","d_model.load_weights('cgan_discriminator.hdf5')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"YFrNJXrQdsmR"},"outputs":[],"source":["# example of loading the generator model and generating images\n","def generateNewSamples():\n","\tn_samples_perclass = 150\n","\tn_classes          = numberOfClasses\n","\t\n","\t# generate points in latent space as input for the generator\n","\tdef generate_latent_points(latent_dim, n_samples_perclass, n_classes=n_classes):\n","\t\tnumberOfsamples = n_samples_perclass*n_classes\n","\t\t# generate points in the latent space\n","\t\tx_input = randn(latent_dim * numberOfsamples)\n","\t\t# reshape into a batch of inputs for the network\n","\t\tz_input = x_input.reshape(numberOfsamples, latent_dim)\n","\t\t# generate labels\n","\t\tonehot = np.zeros((n_classes*n_samples_perclass,1,16))\n","\t\tlabels = np.ndarray((n_classes*n_samples_perclass,)) \n","\t\tfor ii in range(n_classes):\n","\t\t\tfor jj in range(n_samples_perclass):\n","\t\t\t\tonehot[ii*n_samples_perclass+jj,0,ii] = 1\n","\t\t\t\tlabels[ii*n_samples_perclass+jj] = ii\n","\t\treturn [z_input, labels, onehot]\n","\t\n","\n","\t# load model\n","\tmodel = load_model('cgan_generator.hdf5')\n","\t# generate images\n","\tlatent_points, labels, onehot = generate_latent_points(latent_dim=100, n_samples_perclass=n_samples_perclass)\n","\n","\t# generate images\n","\tX1  = model.predict([latent_points, onehot])\n","\t#X1  = inv_scaling(X1,max,min)\n","\tmygenerateddata = []\n","\tmygenerateddata.append(X1)\n","\tmygenerateddata.append(labels)\n","\tmygenerateddata.append(onehot)\n","\n","\t#print(X1.shape)\n","\tfor ii in range(n_samples_perclass*n_classes):\n","\t\tprint(\"label: \",labels[ii])\n","\t\tplt.plot(X1[ii,:,0,0])\n","\t\tplt.plot(X1[ii,:,1,0])\n","\t\tplt.show()\n","\t\tprint(\"******************\")\n","\n","\treturn mygenerateddata"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"f8hUSPAmDlnB"},"outputs":[],"source":["mygenerateddata = generateNewSamples()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Itr4legrPzA"},"outputs":[],"source":["mygenerateddata_inv = mygenerateddata\n","mygenerateddata_inv[0][:,:,0,0] = scx0.inverse_transform(mygenerateddata[0][:,:,0,0])\n","mygenerateddata_inv[0][:,:,1,0] = scx1.inverse_transform(mygenerateddata[0][:,:,1,0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zwbrpB_wrPzA"},"outputs":[],"source":["import pickle\n","pickle.dump(mygenerateddata_inv,open(\"generated_inv01.pickle\",\"wb\"))\n","mygenerateddata_inv[0].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HxYrVpGyrPzA"},"outputs":[],"source":["plt.plot(mygenerateddata_inv[0][0:100,:,1,0])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"NHhLMMZC5uq8"},"outputs":[],"source":["def mergeDatasets():\n","    print(\"generateddata shape: \",mygenerateddata[0].shape)\n","    print(\"original data shape before merging: \",dataset[0].shape)\n","\n","    mygenerateddata[2] = mygenerateddata[2].squeeze()\n","\n","    list_data_base = [dataset,mygenerateddata]\n","    total_data = len(dataset[0]) + len(mygenerateddata[0])\n","\n","    ## Merging the Generated data set to the existing one\n","    newdataset = np.ndarray((total_data,dataset[0].shape[1],dataset[0].shape[2],dataset[0].shape[3]))\n","    newlabels  = np.ndarray((total_data,))\n","    newonehot  = np.ndarray((total_data,16))\n","\n","    newdataset[0:len(dataset[0])] = dataset[0]\n","    newdataset[len(dataset[0]):] = mygenerateddata[0]\n","\n","    newlabels[0:len(dataset[1])] = dataset[1]\n","    newlabels[len(dataset[1]):] = mygenerateddata[1]\n","\n","    newonehot[0:len(dataset[2])] = dataset[2]\n","    newonehot[len(dataset[2]):] = mygenerateddata[2]\n","\n","    print(\"dataset shape after merging: \",newdataset.shape)\n","    datasetNew = [newdataset,newlabels,newonehot]\n","    return datasetNew\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xDt8zwsudsmS"},"outputs":[],"source":["MergedDataset = mergeDatasets()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WK75yD0xrPzB"},"outputs":[],"source":["list_classLabels,counterTrue,counterFalse=postprocess(classifier,dataset)\n","list_classLabelsGen,counterTrue,counterFalse=postprocess(classifier,mygenerateddata)\n","list_classLabelsMerged,counterTrue,counterFalse=postprocess(classifier,MergedDataset)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mybLXMrlrPzB"},"outputs":[],"source":["print(\"Generated Classes after Conditional GAN training\")\n","plt.scatter(np.arange(len(list_classLabelsGen)),mygenerateddata[1])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"du7UpsQrrPzB"},"outputs":[],"source":["print(\"Generated Classes Determined by the Classifier Trained with Original Dataset\")\n","plt.scatter(np.arange(len(list_classLabelsGen)),list_classLabelsGen)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"mEubcd3yrPzC"},"source":["SonuÃ§lar incelendiÄinde:\n","    - Her sÄ±nÄ±ftan eÅit sayÄ±da sÄ±nÄ±f oluÅturmasÄ± gerekirken bunu yapmadÄ±ÄÄ±,\n","    - Conditional GAN'Ä±n 5, 7, ve 8. sÄ±nÄ±flardan yeni sÄ±nÄ±f oluÅturmada zorlandÄ±ÄÄ±,\n","    - oluÅturulan sÄ±nÄ±flarÄ±n classifier tarafÄ±ndan sÄ±nÄ±flandÄ±rÄ±ldÄ±ÄÄ±nda, farklÄ± label'lÄ± sÄ±nÄ±f olarak gÃ¶rdÃ¼ÄÃ¼\n","anlaÅÄ±lmakta.\n","\n","Ancak, yine de kontrolsÃ¼z de olsa, birden fazla sÄ±nÄ±f iÃ§in yeni veri Ã¼retebilmiÅ gibiyiz. Bunu test etmek iÃ§in knn vb yÃ¶ntemleri de kullanarak sÄ±nÄ±flandÄ±rma yapalÄ±m."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jz71h5crrPzJ"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hj6tWf7ndsmW"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Copy of cdcgan_RF.ipynb","private_outputs":true,"provenance":[{"file_id":"1SzdS1Gs_iATaw9ZAUvZH6Lef0qXS_BOv","timestamp":1650436920712}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"nbTranslate":{"displayLangs":["*"],"hotkey":"alt-t","langInMainMenu":true,"sourceLang":"en","targetLang":"fr","useGoogleTranslate":true},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":true}},"nbformat":4,"nbformat_minor":0}