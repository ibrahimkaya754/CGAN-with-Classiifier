{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/ibrahimkaya754/GAN/blob/main/dcgan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"e1_Y75QXJS6h"},"source":["### Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dm3J1J7wsKW3"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VZVgc00ds31l"},"outputs":[],"source":["cd drive/MyDrive/ESEN/Scripts/CGAN_with_Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bH7BE3z29NYI"},"outputs":[],"source":["## MODULES AND THE DATA IMPORT\n","from numpy import expand_dims\n","from numpy import zeros\n","from numpy import ones\n","from numpy.random import randn\n","from numpy.random import randint\n","from keras.datasets.fashion_mnist import load_data\n","from tensorflow.keras.optimizers import *\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.initializers import *\n","import tensorflow as tf\n","import numpy as np\n","import pickle\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import MinMaxScaler\n","from numpy import asarray\n","from numpy.random import randn\n","from numpy.random import randint\n","from keras.models import load_model\n","from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.constraints import Constraint\n","from tensorflow.keras.losses import *\n","#from MoGeNA import *\n","from sklearn.preprocessing import *\n","from tensorflow.python.platform import build_info as tf_build_info\n","from scipy import spatial\n","import os\n","print(tf_build_info.build_info)"]},{"cell_type":"code","source":["os.getcwd()"],"metadata":{"id":"DYhH58NLw7Sm"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gez9Mqf4rPys"},"outputs":[],"source":["gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n","for device in gpu_devices:\n","    tf.config.experimental.set_memory_growth(device, True)\n","    print(\"tensorflow version is: \",tf.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V6eiuGf01S4E"},"outputs":[],"source":["# Load The Data\n","def load_wifi_data(dataDict):\n","  for key in dataDict.keys():\n","    print(\"Preparing %s data\" % (key))\n","    data_wifi   = pickle.load(open(dataDict[key][\"datalocation\"],\"rb\"))\n","    labels_wifi = pickle.load(open(dataDict[key][\"labellocation\"],\"rb\"))\n","    \n","    labels  = []\n","    \n","    print(\"data_shape for %s is %s: \" % (key,data_wifi.shape)) \n","    print('label shape for %s is %s: ' % (key,labels_wifi.shape))\n","\n","    for lbl in labels_wifi:\n","        if lbl not in labels:\n","            labels.append(lbl)\n","\n","    print(\"labels: \",labels)\n","    dataDict[key]['data']   = data_wifi\n","    dataDict[key]['labels'] = labels_wifi\n","\n","    dumbarray = np.zeros((dataDict[key]['labels'].shape[0],16),dtype=np.int16)\n","    for ii in range(dataDict[key]['labels'].shape[0]):\n","        indice = labels_wifi[ii]\n","        dumbarray[ii,indice] = 1\n","\n","    dataDict[key]['label1hot']       = dumbarray\n","    dataDict[key]['numberOfClasses'] = len(labels)\n","    print(\"number of classes: \",dataDict[key]['numberOfClasses'])\n","\n","    dataDict[key][\"data\"]= dataDict[key]['data'].reshape(dataDict[key]['data'].shape[0],\n","                                                         dataDict[key]['data'].shape[1],\n","                                                         dataDict[key]['data'].shape[2],1)\n","    print('dataset shape: ', dataDict[key][\"data\"].shape)\n","    print('max value before scaling: ', np.max(dataDict[key][\"data\"]))\n","    print('min value before scaling: ', np.min(dataDict[key][\"data\"]))\n","    print(\"*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--\")\n","  return dataDict"]},{"cell_type":"code","source":["data={\"TrainData\"      :{\"datalocation\"  : \"../newDATA/trainingData_13042022 15:56:46\",\n","                         \"labellocation\" : \"../newDATA/trainingLabel_13042022 15:56:46\"},\n","      \"ValidationData\" :{\"datalocation\"  : \"../newDATA/knowntestData_13042022 15:56:46\",\n","                         \"labellocation\" : \"../newDATA/knowntestLabel_13042022 15:56:46\"},\n","      \"UnknownData\"    :{\"datalocation\"  : \"../newDATA/unknowntestData_18042022 15:36:59\",\n","                         \"labellocation\" : \"../newDATA/unknowntestLabel_18042022 15:36:59\"}}"],"metadata":{"id":"VNgbC0V-yaIL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = load_wifi_data(data)"],"metadata":{"id":"5wIIbRt10uRl"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zyu1i01WrPyy"},"outputs":[],"source":["def scaling(dataDict,scalingA2=\"TrainData\"):\n","\n","  scalerx0 = MinMaxScaler((-0.5,0.5))\n","  scalerx1 = MinMaxScaler((-0.5,0.5))\n","\n","  scx0 = scalerx0.fit(dataDict[scalingA2][\"data\"][:,:,0,0])\n","  scx1 = scalerx1.fit(dataDict[scalingA2][\"data\"][:,:,1,0])\n","\n","  for key in dataDict.keys():\n","    dataDict[key][\"dataScaled\"] = dataDict[key][\"data\"]\n","    dataDict[key][\"dataScaled\"][:,:,0,0]= scx0.transform(dataDict[key][\"data\"][:,:,0,0])\n","    dataDict[key][\"dataScaled\"][:,:,1,0]= scx1.transform(dataDict[key][\"data\"][:,:,1,0])\n","    print('min, max value of I after scaling %s dataset: %.3f, %.3f' % (key,np.min(dataDict[key][\"dataScaled\"][:,:,0,0]),\n","                                                                              np.max(dataDict[key][\"dataScaled\"][:,:,0,0])))\n","    print('min, max value of Q after scaling %s dataset: %.3f, %.3f' % (key,np.min(dataDict[key][\"dataScaled\"][:,:,1,0]),\n","                                                                              np.max(dataDict[key][\"dataScaled\"][:,:,1,0])))\n","\n","  \n","  return dataDict,[scx0,scx1]\n","data,[scx0,scx1] = scaling(data)"]},{"cell_type":"code","source":["def L2normalize(arg):\n","  import keras.backend as K\n","  arg = K.l2_normalize(arg,axis=1)\n","  return arg"],"metadata":{"id":"bZm60SGUqAay"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class model():\n","  def __init__(self, dataDict, AppendOptimizedParams= True,\n","               location= \"../CGAN_with_Classifier_Models/\",\n","               in_shape=(1024,2,1), loadSavedModel= True,\n","               filters=[64,32,16,8], neurons=50, n_classes=16,\n","               strides=[(2,1),(2,2),(2,2),(2,2)], batchSize = 64,\n","               alpha= 0.2, kernelSize= 3, Drop= 0.2, epochs = 50,\n","               learningRate= 0.001, name=\"Module1\",\n","               best_params= [211.0776,207.0222,235.648,\n","                             129.7916,232.7917,0.3253,\n","                             6.730,0.237,0.0002942,1024,5]):\n","    self.in_shape       = in_shape\n","    self.n_classes      = n_classes\n","    self.filters        = filters\n","    self.neurons        = neurons\n","    self.strides        = strides\n","    self.alpha          = alpha\n","    self.kernelSize     = kernelSize\n","    self.Drop           = Drop\n","    self.learningRate   = learningRate\n","    self.modelName      = name\n","    self.data           = dataDict\n","    self.batchSize      = batchSize\n","    self.epochs         = epochs\n","    self.location       = location\n","    self.loadSavedModel = loadSavedModel\n","    self.bestParams     = best_params\n","    self.AppendOptimizedParams = AppendOptimizedParams\n","    self.models         = {}\n","\n","    if self.AppendOptimizedParams:\n","      self.__AppendOptimumPArams()\n","    \n","    self.CallBack()\n","    self.define_classifier()\n","    self.createSiameseModel()\n","\n","  def define_classifier(self):\n","    self.opt = Adam(lr=self.learningRate)\n","    in_image = Input(shape=self.in_shape)\n","    layer1   = Reshape((64,32,1))(in_image)\n","    \n","    for fltr,strd in zip(self.filters,self.strides):\n","        layer1 = Conv2D(int(round(fltr)), (int(round(self.kernelSize)),int(round(self.kernelSize))), \n","                        strides=strd, padding='same')(layer1)\n","        layer1 = LeakyReLU(alpha=self.alpha)(layer1)\n","\n","    flatten                 = Flatten()(layer1)\n","    layer1                  = Dropout(self.Drop)(flatten)\n","    featuredescriptor       = Dense(int(round(self.neurons)))(layer1)\n","    #featuredescriptor      = Lambda(L2normalize)(featuredescriptor)\n","    layer1                  = LeakyReLU(alpha=self.alpha)(featuredescriptor)\n","    layer1                  = Dropout(self.Drop)(layer1)\n","    out                     = Dense(16,activation='softmax')(layer1)\n","\n","    self.classifier  = Model(in_image, out)\n","    \n","    if self.loadSavedModel:\n","      self.classifier.load_weights(self.location+self.modelName+\".hdf5\")\n","    \n","    print(\"NETWORK SUMMARY\")\n","    print(\"--------------------------------------\")\n","    self.classifier.summary()\n","\n","    tf.keras.utils.plot_model(self.classifier,show_shapes=True)\n","\n","    self.siam1 = Model(in_image,featuredescriptor)\n","    self.siam2 = Model(in_image,featuredescriptor)\n","\n","    self.models[self.modelName]= {\"ClassifierModel\": self.classifier,\n","                                  \"Location\"       : self.location+self.modelName+\".hdf5\"}\n","\n","  def trainClassifier(self,isTrain=True,epochs=50,batchSize=1024):\n","    self.epochs    = epochs\n","    self.batchSize = batchSize\n","    self.classifier.compile(loss='categorical_crossentropy', optimizer=self.opt, metrics=['accuracy'])\n","    if isTrain:\n","      self.history = self.models[self.modelName][\"ClassifierModel\"].fit(data['TrainData'][\"dataScaled\"], \n","                                                                        data['TrainData'][\"label1hot\"], \n","                                                                        batch_size=int(round(self.batchSize)), \n","                                                                        epochs=int(round(self.epochs)), \n","                                                                        validation_data=(data['ValidationData'][\"dataScaled\"],\n","                                                                                         data['ValidationData'][\"label1hot\"]), \n","                                                                        verbose=1, shuffle=True,\n","                                                                        callbacks=[self.callbacks[\"checkpoint\"], \n","                                                                                   self.callbacks[\"reduce_lr\"]])\n","      self.models[self.modelName][\"modelhistory\"] = self.history \n","                                    \n","  def CallBack(self):\n","    self.callbacks = {}\n","    self.callbacks[\"checkpoint\"]     =  ModelCheckpoint(filepath=self.location+self.modelName+\".hdf5\", \n","                                        monitor='val_accuracy', verbose=1, \n","                                        save_best_only=True, period=1, \n","                                        mode='max',save_weights_only=False)\n","    self.callbacks[\"reduce_lr\"]      =  ReduceLROnPlateau(monitor='val_accuracy', \n","                                        factor=0.98,\n","                                        patience=10, \n","                                        min_lr=0.0000001, mode='max', verbose=1)\n","    self.callbacks[\"early_stopping\"] =  EarlyStopping(monitor='val_accuracy', \n","                                                      patience=250, verbose=1, \n","                                                      mode='auto')\n","  \n","  ## __AppendOptimumPArams appends the optimum parameters obtained in GA to network                                                     \n","  def __AppendOptimumPArams(self):\n","    ## This function appends the optimum values \n","    ## obtained from GA to the classifier model.\n","    self.filters      = self.bestParams[0:4]\n","    self.neurons      = self.bestParams[4]\n","    self.alpha        = self.bestParams[5]\n","    self.kernelSize   = self.bestParams[6]\n","    self.Drop         = self.bestParams[7]\n","    self.learningRate = self.bestParams[8]\n","    self.batchSize    = int(round(self.bestParams[9]))\n","    self.epochs       = int(round(self.bestParams[10]))\n","    print(\"\\nOPTIMUM PARAMETER SET APPENDED\")\n","    print(\"--------------------------------------\")\n"," \n","  ## __distance just calculates the element-wise absolute difference of two tensors\n","  def __distance(self,args):\n","    tensor1,tensor2 = args\n","    dist = tf.abs((tensor1 - tensor2))\n","    return dist\n","  \n","  def createSiameseModel(self):\n","    inp1     = Input(shape=(1024,2,1),name=\"Siaminput1\")\n","    inp2     = Input(shape=(1024,2,1),name=\"Siaminput2\")\n","    d1out    = self.siam1(inp1)\n","    d2out    = self.siam2(inp2)\n","    l1dist   = Lambda(self.__distance)([d1out,d2out])\n","    l1dist   = Lambda(L2normalize)(l1dist)\n","    out      = Dense(1,activation=\"sigmoid\")(l1dist)\n","    self.Siam = Model([inp1,inp2],out)\n","    self.Siam.compile(loss='binary_crossentropy', optimizer=self.opt, metrics=['accuracy'])\n","    \n","    print(\"\\n\\nSIAM NETWORK SUMMARY\")\n","    print(\"--------------------------------------\")\n","    self.Siam.summary()\n","    self.models[self.modelName][\"SiamModel\"] = self.Siam\n","  \n","  ## __createSiamDataCouples creates data couples for siamese network training\n","  def __createSiamDataCouples(self,batchSize,dataset):\n","    labels_siamese = list(np.ones(batchSize))\n","    siamese_inp1   = []\n","    siamese_inp2   = []\n","    indiceCouples  = np.random.randint(0,len(dataset[\"dataScaled\"]),(batchSize,2))\n","    for ii in range(batchSize):\n","      indice1 = indiceCouples[ii,0]\n","      indice2 = indiceCouples[ii,1]\n","      siamese_inp1.append(dataset[\"dataScaled\"][indice1])\n","      siamese_inp2.append(dataset[\"dataScaled\"][indice2])\n","      if dataset[\"labels\"][indice1]==dataset[\"labels\"][indice2]:\n","        labels_siamese[ii] = 0.0\n","    return np.array(siamese_inp1),np.array(siamese_inp2),np.array(labels_siamese)\n","\n","  def trainSiamese(self, Nepochs=100, batchSize=512,\n","                   trainFeatureExtractors= False, \n","                   load_saved_data=False):   \n","      \n","    batchPerEp    = int(self.data[\"TrainData\"][\"dataScaled\"].shape[0] / batchSize)\n","\n","    if load_saved_data:\n","      self.siam1.load_weights(self.location+self.modelName+'_siam1.hdf5')\n","      self.siam2.load_weights(self.location+self.modelName+'_siam2.hdf5')\n","      self.Siam.load_weights(self.location+self.modelName+'_siamModelUnited.hdf5')\n","\n","    # We make the extracted features from classifier non-trainable here.\n","    if trainFeatureExtractors:\n","      self.siam1.trainable = True\n","      self.siam2.trainable = True\n","    else:\n","      self.siam1.trainable = False\n","      self.siam2.trainable = False\n","\n","    # manually enumerate epochs\n","    for i in range(Nepochs):\n","      if i%25 == 0:\n","        # save the generator model\n","        self.siam1.save(self.location+self.modelName+'_siam1.hdf5')\n","        self.siam2.save(self.location+self.modelName+'_siam2.hdf5')\n","        self.Siam.save(self.location+self.modelName+'_siamModelUnited.hdf5')\n","      # enumerate batches over the training set\n","      for j in range(batchPerEp):\n","        # get randomly selected samples, train and calculate loss\n","        siamese_inp1,siamese_inp2,labels_siamese = self.__createSiamDataCouples(batchSize, self.data[\"TrainData\"])\n","        loss, _ = self.Siam.train_on_batch([siamese_inp1,siamese_inp2], labels_siamese)\n","        \n","        # calculate validation loss\n","        siamese_inp1_val,siamese_inp2_val,labels_siamese_val = self.__createSiamDataCouples(batchSize, self.data[\"ValidationData\"])\n","        outval   = np.squeeze(self.Siam([siamese_inp1_val,siamese_inp2_val]))\n","        bce      = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","        val_loss = bce(labels_siamese_val, outval).numpy()\n","\n","        print('epoch: %d/%d, %d/%d, loss: %.3f, valloss: %.3f' % (i+1, Nepochs, j+1, batchPerEp, loss, val_loss))\n","      print(\"-----------------------------------------------\")\n","\n","    # save the generator model\n","    self.siam1.save(self.location+self.modelName+'_siam1.hdf5')\n","    self.siam2.save(self.location+self.modelName+'_siam2.hdf5')\n","    self.Siam.save(self.location+self.modelName+'_siamModelUnited.hdf5')\n"],"metadata":{"id":"1xYJchqHiwcA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mymodel = model(data,name=\"Module1\", loadSavedModel=True, AppendOptimizedParams=True)"],"metadata":{"id":"hkeLjGP3iwzr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mymodel.data[\"TrainData\"].keys()"],"metadata":{"id":"zD1CRbWiSYaZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mymodel.__dict__.keys()"],"metadata":{"id":"GTexw4QYiw7d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mymodel.trainClassifier(isTrain=True,epochs=1)"],"metadata":{"id":"2KWgr7oV3snU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mymodel.trainSiamese(Nepochs=10, batchSize=512,\n","                     trainFeatureExtractors= False, \n","                     load_saved_data=False)"],"metadata":{"id":"jlIV-URcrEuR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mymodel.models[\"Module1\"]"],"metadata":{"id":"JgQ7ZJjtpIPQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Below is the section to find Cosine Similarity (CS) of the Classes.\n","\n","The CS is calculated for every class pairs, e.g. 0-0, 0,1, 0,2, etc.\n","\n","The dictionary \"dict_mean_cos\" is the dictionary keeping all the values for CS."],"metadata":{"id":"NUqCVIqeeJF9"}},{"cell_type":"code","source":["## We define CS4OriginalData function to make the calculation for CS for all class pairs.\n","## The dictionary \"dict_mean_cos\" is created inside this function.\n","def CS4OriginalData(NumberOfDataPickedRandomly,model,dataset1,dataset2):\n","  siam1            = model\n","  siam2            = modelDict[\"siam2\"] \n","  siamUnited       = modelDict[\"siamModelUnited\"] \n","  nOfClass1        = dataset1[2].shape[1]\n","  dict_mean_cos1   = {str(ii): {\"indices\":[]} for ii in range(nOfClass1)}\n","  nOfClass2        = dataset2[2].shape[1]\n","  dict_mean_cos2   = {str(ii): {\"indices\":[]} for ii in range(nOfClass2)}\n","  total_DataNumber1= 0\n","  total_DataNumber2= 0\n","\n","  for key in dict_mean_cos1.keys():\n","    for ii in range(len(dataset1[1])):\n","      if dataset1[1][ii] == int(key):\n","        dict_mean_cos1[key][\"indices\"].append(ii)\n","\n","  for key in dict_mean_cos2.keys():\n","    for ii in range(len(dataset2[1])):\n","      if dataset2[1][ii] == int(key):\n","        dict_mean_cos2[key][\"indices\"].append(ii)\n","\n","\n","  for key in dict_mean_cos1.keys():\n","    dict_mean_cos1[key][\"shape\"] = dataset1[1][dict_mean_cos1[key][\"indices\"]].shape[0]\n","    print(\"number of data existing for each class %s: %d for dataset1\" % (key,dict_mean_cos1[key][\"shape\"]))\n","    total_DataNumber1 = total_DataNumber1 + dict_mean_cos1[key][\"shape\"]\n","  \n","  print(\"----------------------------------------------------------------------------------------\\n\")\n","  \n","  for key in dict_mean_cos2.keys():\n","    dict_mean_cos2[key][\"shape\"] = dataset2[1][dict_mean_cos2[key][\"indices\"]].shape[0]\n","    print(\"number of data existing for each class %s: %d for dataset2\" % (key,dict_mean_cos2[key][\"shape\"]))\n","    total_DataNumber2 = total_DataNumber2 + dict_mean_cos2[key][\"shape\"]\n","\n","\n","  print(\"----------------------------------------------------------------------------------------\\n\")\n","  print(\"total_data_number for dataset1: %d\" % (total_DataNumber1))\n","  print(\"dataset1 shape is: \", dataset1[1].shape[0])\n","  print(\"----------------------------------------------------------------------------------------\")\n","  print(\"total_data_number for dataset2: %d\" % (total_DataNumber2))\n","  print(\"dataset2 shape is: \", dataset2[1].shape[0])\n","  print(\"----------------------------------------------------------------------------------------\\n\")\n","  for key in dict_mean_cos1.keys():\n","    try:\n","      dict_mean_cos1[key][\"siam1_Predictions\"] = siam1.predict(dataset1[0][dict_mean_cos1[key][\"indices\"]])\n","    except:\n","      continue\n","  \n","  for key in dict_mean_cos2.keys():\n","    try:\n","      dict_mean_cos2[key][\"siam2_Predictions\"] = siam2.predict(dataset2[0][dict_mean_cos2[key][\"indices\"]])\n","    except:\n","      continue\n","\n","  for key1 in dict_mean_cos1.keys():\n","    len1 = dict_mean_cos1[key1][\"shape\"]\n","    for key2 in dict_mean_cos2.keys():\n","      len2 = dict_mean_cos2[key2][\"shape\"]   \n","      \n","      try:\n","        list_cs   = []\n","        list_Siam = []\n","        lenmin    = np.minimum(len1,len2)\n","        \n","        for ii in range(NumberOfDataPickedRandomly):\n","          ind1, ind2 = np.random.randint(0,lenmin,2)\n","          ##### HERE IS FOR COSINE SIMILARITY ##########\n","          cosinesimilarity = 1 - spatial.distance.cosine(dict_mean_cos1[key1][\"siam1_Predictions\"][ind1], \n","                                                         dict_mean_cos2[key2][\"siam2_Predictions\"][ind2])\n","          list_cs.append(cosinesimilarity)\n","\n","          ##### HERE IS FOR SIAMESE SIMILARITY ##########\n","          siameseResult = siamUnited([dataset1[0][dict_mean_cos1[key1][\"indices\"][ind1]].reshape(1,1024,2,1),\n","                                      dataset2[0][dict_mean_cos2[key2][\"indices\"][ind2]].reshape(1,1024,2,1)])\n","          list_Siam.append(1 - siameseResult)\n","        \n","        string1 = \"mean cosine similarity \"\n","        string2 = \"mean siamese result\"\n","        print(\"%s is: %.3f and %s is: %.3f for %s and %s\" % (string1,np.mean(list_cs),\n","                                                             string2,np.mean(list_Siam),\n","                                                             key1,key2))\n","      except:\n","        continue\n","    \n","    print(\"---------------------------------------------------------------------------------------\")\n","  return dict_mean_cos1,dict_mean_cos2"],"metadata":{"id":"coLzQm_-iTzS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def OptimumDataPickNumber(list_number=[50]):\n","  dict_result_OptimizedClassifier1 = {}\n","  dict_result_OptimizedClassifier2 = {}\n","  for RandomDataNo in list_number:\n","    print(\"RUNNING FOR OptimizedClassifier1\")\n","    _,_ = CS4OriginalData(RandomDataNo,OptimizedClassifier1,dataset,dataset)\n","    print(\"RUNNING FOR OptimizedClassifier2\")\n","    _,_ = CS4OriginalData(RandomDataNo,OptimizedClassifier2,dataset,dataset)\n","  return dict_result_OptimizedClassifier1, dict_result_OptimizedClassifier2\n","  \n","_,_ = OptimumDataPickNumber()"],"metadata":{"id":"3RvFEFvHiUGC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Here We write the results to pickle.\n","#unitedResult= [dict_result_OptimizedClassifier1, dict_result_OptimizedClassifier2]\n","#pickle.dump(unitedResult,open(\"dict_result_opt.pickle\",\"wb\"))"],"metadata":{"id":"eVnokZ-RuQ0H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def OptimumDataPickNumber(list_number=[50]):\n","  dict_result_OptimizedClassifier1 = {}\n","  dict_result_OptimizedClassifier2 = {}\n","  for RandomDataNo in list_number:\n","    print(\"RUNNING FOR OptimizedClassifier1\")\n","    _,_ = CS4OriginalData(RandomDataNo,OptimizedClassifier1,dataset,datasetVal)\n","    print(\"RUNNING FOR OptimizedClassifier2\")\n","    _,_ = CS4OriginalData(RandomDataNo,OptimizedClassifier2,dataset,datasetVal)\n","  return dict_result_OptimizedClassifier1, dict_result_OptimizedClassifier2\n","  \n","_,_ = OptimumDataPickNumber()"],"metadata":{"id":"WnbB0oc0lLNj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def OptimumDataPickNumber(list_number=[50]):\n","  dict_result_OptimizedClassifier1 = {}\n","  dict_result_OptimizedClassifier2 = {}\n","  for RandomDataNo in list_number:\n","    print(\"RUNNING FOR OptimizedClassifier1\")\n","    _,_ = CS4OriginalData(RandomDataNo,OptimizedClassifier1,dataset,datasetVal_unknown)\n","    print(\"RUNNING FOR OptimizedClassifier2\")\n","    _,_ = CS4OriginalData(RandomDataNo,OptimizedClassifier2,dataset,datasetVal_unknown)\n","  return dict_result_OptimizedClassifier1, dict_result_OptimizedClassifier2\n","  \n","_,_ = OptimumDataPickNumber()"],"metadata":{"id":"ZHNWfymPlLkB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"TVCZ-RuzlLmL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## BELOW IS THE PART WHERE WE RUN GENETIC ALGORITM OPTIMIZER"],"metadata":{"id":"Sbqbi_KkAxRD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"asQqcGO3rPyz"},"outputs":[],"source":["# Function for MoGenA\n","def optClassifier(inp):\n","    print(\"parameter set is: \",inp)\n","    filters      = inp[0:4]\n","    neurons      = inp[4]\n","    alpha        = inp[5]\n","    kernelSize   = inp[6]\n","    Drop         = inp[7]\n","    learningRate = inp[8]\n","    classifier   = define_classifier(in_shape=(9,512,1), n_classes=14, \n","                                     filters=filters, neurons=neurons,\n","                                     strides=[(2,1),(2,2),(2,2),(2,2)],\n","                                     alpha= alpha, kernelSize= kernelSize, Drop= Drop,\n","                                     learningRate= learningRate)\n","    history      = classifier.fit(dataset[0], dataset[2], \n","                                  batch_size=int(round(inp[9])), \n","                                  epochs=int(round(inp[10])), \n","                                  validation_data=(datasetVal[0],datasetVal[2]), \n","                                  #validation_split = 0.25,\n","                                  verbose=1, shuffle=True,\n","                                  callbacks=[checkpoint, reduce_lr])\n","    minimization_param = -np.max(history.history['val_accuracy'])\n","    return minimization_param"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5H8FKK1RrPyz","scrolled":true},"outputs":[],"source":["# Genetic Algorithm\n","runOpt = False\n","if runOpt:\n","  ga = Mogena.MoGenA(fitness_func         = optClassifier, \n","                    number_of_generation = 20,\n","                    number_of_variables  = 11,\n","                    population_size      = 30,\n","                    lower_boundaries     = [8  ,8  ,8  ,8  ,8  ,0.1,3,0.1,0.00001,64  ,20 ],\n","                    upper_boundaries     = [250,250,250,250,250,0.5,7,0.5,0.001  ,1024,200],\n","                    use_saved_data       = False)\n","  result =  {'best_fitness_ever'           : ga.best_fitness_ever,\n","            'best_individual_decoded_ever' : ga.best_individual_decoded_ever}"]},{"cell_type":"code","source":[""],"metadata":{"id":"OMjIGA6YlLoj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"DC9E3ZukiUKJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"KXPSzRbAiUNi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Below is the section where we load the synthetically generated data and compare it with the original one using cosine similarity."],"metadata":{"id":"bu6Yzw9dfcIf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jw-e7ih6xzcA"},"outputs":[],"source":["def load_synthetic_data(data_location,classifier=OptimizedClassifier[\"classifier\"]):\n","  synthetic_data   = pickle.load(open(data_location,\"rb\"))\n","  synthetic_data[:,:,0]= scx0.transform(synthetic_data[:,:,0])\n","  synthetic_data[:,:,1]= scx1.transform(synthetic_data[:,:,1])\n","  print(\"Synthetic Data Shape is: \", synthetic_data.shape)\n","  synthetic_data_result = classifier.predict(synthetic_data)\n","  listClassifiedSyntheticData = [np.argmax(synthetic_data_result[ii]) for ii in range(len(synthetic_data))]\n","  print(\"Generated Classes are: \", listClassifiedSyntheticData)\n","  return synthetic_data, listClassifiedSyntheticData\n","synthetic_data, listClassifiedSyntheticData= load_synthetic_data(\"../mygenerateddata_normal2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XJ07D_Cu0Yoo"},"outputs":[],"source":["## CompareCS4SynData is to compare the cosine similarity for the synthetic (syn) data and the original data.\n","## We compare the synthetic data classes determined by the classifier with the original classes existing in the original dataset\n","def CompareCS4SynData(synthetic_data,listClassifiedSyntheticData,siam1,siam2,dict_mean_cos):\n","  res2 = siam2.predict(synthetic_data)\n","  for key,value in enumerate(listClassifiedSyntheticData):\n","    res1 = siam1.predict(dataset[0][dict_mean_cos[str(value)][\"indices\"]])\n","    list_cs = []\n","    for ii in range(100):\n","      ind1 = np.random.randint(0,2500,1)\n","      cosinesimilarity = 1- spatial.distance.cosine(res1[ind1], res2[key])\n","      #print(cosinesimilarity)\n","      list_cs.append(cosinesimilarity)\n","    string = \"mean cosine similarity for class %s and 'newly generated class %s'\" % (value,value)\n","    print(\"%s is: %.3f\" % (string,np.mean(list_cs)))\n","\n","CompareCS4SynData(synthetic_data,listClassifiedSyntheticData,\n","                  OptimizedClassifier[\"siam1\"],OptimizedClassifier[\"siam2\"],\n","                  dict_mean_cos=dict_result_opt[\"50\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G2tj49Kk0Yqo"},"outputs":[],"source":["plt.scatter(listClassifiedSyntheticData,np.arange(len(listClassifiedSyntheticData)))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6YmXmyZGBFAq"},"outputs":[],"source":["# Postprocess\n","def postprocess(model,dataset):\n","    OptimizedClassifier[\"classifier\"].load_weights(\"./classifier.hdf5\")\n","    results = model.predict(dataset[0])\n","    list_class = []\n","    for ii in range(results.shape[0]):\n","        list_class.append(np.argmax(results[ii]))\n","\n","    counterTrue  = 0\n","    counterFalse = 0\n","    for ii in range(len(list_class)):\n","        if list_class[ii] == dataset[1][ii]:\n","            counterTrue = counterTrue + 1\n","        else:\n","            counterFalse = counterFalse + 1\n","    print(\"True:%d    False:%d\" % (counterTrue,counterFalse))\n","    return list_class,counterTrue,counterFalse"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P8u-DKqKGz0E"},"outputs":[],"source":["list_class,counterTrue,counterFalse=postprocess(OptimizedClassifier[\"classifier\"],datasetVal)"]},{"cell_type":"markdown","source":["## GENERATIVE MODEL (CONDITIONAL GAN)"],"metadata":{"id":"GkFdHFmnA_dB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nkMeyqNcAYAs"},"outputs":[],"source":["class Spectral_Norm(Constraint):\n","    def __init__(self, power_iters=5):\n","        self.n_iters = power_iters\n","        \n","    def l2_normalize(self, x, eps=1e-12):\n","        return x / tf.linalg.norm(x + eps)\n","\n","    def __call__(self, w):\n","        flattened_w = tf.reshape(w, [w.shape[0], -1])\n","        u = tf.random.normal([flattened_w.shape[0]])\n","        v = tf.random.normal([flattened_w.shape[1]])\n","        for i in range(self.n_iters):\n","            v = tf.linalg.matvec(tf.transpose(flattened_w), u)\n","            v = self.l2_normalize(v)\n","            u = tf.linalg.matvec(flattened_w, v)\n","            u = self.l2_normalize(u)\n","        sigma = tf.tensordot(u, tf.linalg.matvec(flattened_w, v), axes=1)\n","        return w / sigma\n","\n","    def get_config(self):\n","        return {'n_iters': self.n_iters}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VFnJtELM9nna"},"outputs":[],"source":["# define the standalone discriminator model\n","def define_discriminator(in_shape=(1024,2,1), n_classes=numberOfClasses, convDim='2D'):\n","    # label input\n","    initializer = tf.keras.initializers.GlorotNormal()\n","    const = Spectral_Norm()\n","    in_label = Input(shape=(1,16))\n","    # embedding for categorical input\n","    li = Embedding(16, 50)(in_label)\n","    # scale up to image dimensions with linear activation\n","    n_nodes = in_shape[0] * in_shape[1]\n","    li = Dense(n_nodes)(li)\n","    if convDim == '1D':\n","        # reshape to additional channel\n","        li = Reshape((in_shape[0], in_shape[1]))(li)\n","        # image input\n","        in_image = Input(shape=in_shape[0:2])\n","        # concat label as a channel\n","        merge = Concatenate()([in_image, li])\n","        merge = Reshape((64,64))(merge)\n","        # downsample\n","        layer1 = Conv1D(128, (3), strides=(2), padding='same',\n","                        kernel_initializer = glorot_normal(),\n","                        kernel_constraint= const)(merge)\n","        layer1 = LeakyReLU(alpha=0.2)(layer1)\n","\n","        layer1 = Conv1D(64, (3), strides=(2), padding='same',\n","                        kernel_initializer = glorot_normal(),\n","                        kernel_constraint= const)(layer1)\n","        layer1 = LeakyReLU(alpha=0.2)(layer1)\n","\n","        layer1 = Conv1D(32, (3), strides=(2), padding='same',\n","                        kernel_initializer = glorot_normal(),\n","                        kernel_constraint= const)(layer1)\n","        layer1 = LeakyReLU(alpha=0.2)(layer1)\n","\n","    else:\n","        leakyReluAlpha = 0.2\n","        ksize = (3,3)\n","        # reshape to additional channel\n","        li = Reshape((in_shape[0], in_shape[1], 16))(li)\n","        # image input\n","        in_image = Input(shape=in_shape)\n","        # concat label as a channel\n","        merge = Concatenate()([in_image, li])\n","        merge = Reshape((64,32,17))(merge)\n","        # downsample\n","        layer1 = Conv2D(128, ksize, strides=(2,1), padding='same',\n","                        kernel_initializer = initializer,\n","                        kernel_constraint= const)(merge)\n","        layer1 = LeakyReLU(alpha=leakyReluAlpha)(layer1)\n","\n","        layer1 = Conv2D(64, ksize, strides=(2,2), padding='same',\n","                        kernel_initializer = initializer,\n","                        kernel_constraint= const)(layer1)\n","        layer1 = LeakyReLU(alpha=leakyReluAlpha)(layer1)\n","\n","        layer1 = Conv2D(32, ksize, strides=(2,2), padding='same',\n","                        kernel_initializer = initializer,\n","                        kernel_constraint= const)(layer1)\n","        layer1 = LeakyReLU(alpha=leakyReluAlpha)(layer1)\n","        \n","        layer1 = Conv2D(16, ksize, strides=(2,2), padding='same',\n","                        kernel_initializer = initializer,\n","                        kernel_constraint= const)(layer1)\n","        layer1 = LeakyReLU(alpha=leakyReluAlpha)(layer1)\n","\n","    layer1 = Flatten()(layer1)\n","    layer1 = Dropout(0.2)(layer1)\n","    # output\n","    out_layer = Dense(1, activation='sigmoid')(layer1)\n","    # define model\n","    model = Model([in_image, in_label], out_layer)\n","    # compile model\n","    opt = RMSprop(lr=0.0001)\n","    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","    model.summary()\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gcZKpierWzHk"},"outputs":[],"source":["# size of the latent space\n","latent_dim = 100"]},{"cell_type":"code","source":["numberOfClasses"],"metadata":{"id":"UQhfC7Rb-q24"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"orYWSZLT9toc"},"outputs":[],"source":["# define the standalone generator model\n","def define_generator(latent_dim, n_classes=numberOfClasses, convDim='2D'):\n","    # label input\n","    initializer = tf.keras.initializers.GlorotNormal()\n","    in_label = Input(shape=(1,16))\n","    # embedding for categorical input\n","    li = Embedding(16, 50)(in_label)\n","    # linear multiplication\n","  \n","    if convDim== '1D':\n","        n_nodes = 8 * 16 \n","        li = Dense(n_nodes)(li)\n","        # reshape to additional channel\n","        li = Reshape((64, 2))(li)\n","        # image generator input\n","        in_lat = Input(shape=(latent_dim,))\n","        # foundation for 7x7 image\n","        n_nodes = 64 * 64\n","        gen = Dense(n_nodes)(in_lat)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","        gen = Reshape((64, 64))(gen)\n","        # merge image gen and label input\n","        merge = Concatenate()([gen, li])\n","          # upsample to 14x14\n","        gen = Conv1DTranspose(16, (4), strides=(4), padding='same',kernel_initializer = glorot_normal())(merge)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","        # upsample to 28x28\n","        gen = Conv1DTranspose(16, (4), strides=(4), padding='same',kernel_initializer = glorot_normal())(gen)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","\n","        gen = Conv1DTranspose(16, (4), strides=(2), padding='same',kernel_initializer = glorot_normal())(gen)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","        # output\n","        gen = Conv1D(1, (4), activation='tanh', padding='same',kernel_initializer = glorot_normal())(gen)\n","        out_layer = Reshape((1024, 2))(gen)\n","    else:\n","        n_nodes = 4 * 4\n","        li = Dense(n_nodes)(li)\n","        # reshape to additional channel\n","        li = Reshape((16, 4*4, 1))(li)\n","        # image generator input\n","        in_lat = Input(shape=(latent_dim,))\n","        # foundation for 7x7 image\n","        n_nodes = 4 * 4 * 16\n","        gen = Dense(n_nodes)(in_lat)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","        gen = Reshape((16, 4*4, 1))(gen)\n","        # merge image gen and label input\n","        merge = Concatenate()([gen, li])\n","\n","        gen = Conv2DTranspose(256, (4,4), strides=(2,2), padding='same',kernel_initializer = initializer)(merge)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","\n","        gen = Conv2DTranspose(128, (4,4), strides=(2,1), padding='same',kernel_initializer = initializer)(gen)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","\n","        gen = Conv2DTranspose(128, (4,4), strides=(1,1), padding='same',kernel_initializer = initializer)(gen)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","\n","        gen = Conv2DTranspose(64, (4,4), strides=(1,1), padding='same',kernel_initializer = initializer)(gen)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","    \n","        gen = Conv2D(1, (3,3), strides=(1,1), activation='tanh', padding='same',\n","                     kernel_initializer = initializer)(gen)\n","        gen = Dropout(0.2)(gen)\n","    out_layer = Reshape((1024, 2, 1))(gen)\n","    # define model\n","    model = Model([in_lat, in_label], out_layer)\n","    model.summary()\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UD1E8McUoppi"},"outputs":[],"source":["# Inception Score for measuring the similarity between the generated signals and the real ones, using KL Divergence\n","def InceptionScore(nelem=100):\n","    for tt in range(1):\n","        RealImageList = []\n","        latent_points, labels = generate_latent_points(latent_dim=100, n_samples=30)\n","        for elem in range(labels.shape[0]):\n","            nolabel = np.argmax(labels[elem])\n","            for ii in range(1000):\n","                if dataset[1][ii] == nolabel:\n","                    indice = ii\n","        \n","            RealImageList.append(dataset[0][indice])\n","\n","        RealImageList= np.array(RealImageList)\n","        X1  = g_model.predict([latent_points, labels])\n","        kl  = KLDivergence()\n","        bce = BinaryCrossentropy(from_logits=True)\n","\n","    return kl(RealImageList,X1).numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v6oBxiZS9wrY"},"outputs":[],"source":["# define the combined generator and discriminator model, for updating the generator\n","def define_gan(g_model, d_model):\n","    # make weights in the discriminator not trainable\n","    #d_model.trainable = False\n","\n","    in_label = Input(shape=(1,16))\n","    in_lat = Input(shape=(latent_dim,))\n","\n","    gen_out = g_model([in_lat,in_label])\n","\n","    gan_out = d_model([gen_out,in_label])\n","\n","    # get noise and label inputs from generator model\n","    #gen_noise, gen_label = g_model.input\n","    # get image output from the generator model\n","    #gen_output = g_model.output\n","    # connect image output and label input from generator as inputs to discriminator\n","    #gan_output = d_model([gen_output, gen_label])\n","\n","    # define gan model as taking noise and label and outputting a classification\n","    model = Model([in_lat,in_label], gan_out)\n","    # compile model\n","    opt = RMSprop(lr=0.0003)\n","    model.compile(loss='binary_crossentropy', optimizer=opt)\n","    #model.compile(loss=Ganloss, optimizer=opt)\n","    model.summary()\n","\n","    return model"]},{"cell_type":"code","source":["dataset[2].shape"],"metadata":{"id":"O9RgHPpo-3NT"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rv02rxjoXSCa"},"outputs":[],"source":["# select real samples\n","def generate_real_samples(dataset, n_samples):\n","\t# split into images and labels\n","\timages, labels, onehotenc = dataset\n","\t# choose random instances\n","\tix = randint(0, images.shape[0], n_samples)\n","\t# select images and labels\n","\tX, labels, onehotenc = images[ix], labels[ix], onehotenc[ix]\n","\tonehotenc = onehotenc.reshape((n_samples,1,16))\n","\t# generate class labels\n","\ty = np.ones((n_samples, 1))\n","\treturn [X, onehotenc], [y, onehotenc]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"trF69d8AXWDi"},"outputs":[],"source":["# generate points in latent space as input for the generator\n","def generate_latent_points(latent_dim, n_samples, n_classes=numberOfClasses):\n","\t# generate points in the latent space\n","\tx_input = randn(latent_dim * n_samples)\n","\t# reshape into a batch of inputs for the network\n","\tz_input = x_input.reshape(n_samples, latent_dim)\n","\t# generate labels\n","\tlabels = randint(0, n_classes, n_samples)\n","\tdumbarray = np.zeros((n_samples,1,16))\n","\tfor ii in range(n_samples):\n","\t\tindice = labels[ii]\n","\t\tdumbarray[ii,0,indice] = 1\n","\treturn [z_input, dumbarray]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kUKaHkbvXZrr"},"outputs":[],"source":["# use the generator to generate n fake examples, with class labels\n","def generate_fake_samples(generator, z_input, labels_input, n_samples):\n","\t# predict outputs\n","\timages = generator.predict([z_input, labels_input])\n","\t# create class labels\n","\ty = np.zeros((n_samples, 1))\n","\treturn [images, labels_input], y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x3ATFLSGrPy-","scrolled":true},"outputs":[],"source":["# create the discriminator\n","d_model = define_discriminator(convDim='2D')\n","tf.keras.utils.plot_model(d_model, show_shapes=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZsL6GlPuV_Dw"},"outputs":[],"source":["# create the generator\n","g_model = define_generator(latent_dim,convDim='2D')\n","tf.keras.utils.plot_model(g_model, show_shapes=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RCofZKnlXHI_","scrolled":true},"outputs":[],"source":["# create the gan\n","gan_model = define_gan(g_model, d_model)\n","tf.keras.utils.plot_model(gan_model,show_shapes=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"12Em1-YedsmR"},"outputs":[],"source":["# train the generator and discriminator\n","def train(g_model, d_model, gan_model, dataset, latent_dim, \n","          n_epochs=100, n_batch=128, load_saved_data=True, \n","          dclip_value=0.1, gclip_value=0.1):\n","    \n","    if load_saved_data:\n","        g_model.load_weights('cgan_generator.hdf5')\n","        d_model.load_weights('cgan_discriminator.hdf5')\n","  \n","    bat_per_epo = int(dataset[0].shape[0] / n_batch)\n","    half_batch = int(n_batch / 2)\n","    # manually enumerate epochs\n","    for i in range(n_epochs):\n","        if i%10 == 0:\n","          # save the generator model\n","            g_model.save('cgan_generator.hdf5')\n","            d_model.save('cgan_discriminator.hdf5')\n","        # enumerate batches over the training set\n","        for j in range(bat_per_epo):\n","            list_z_input = []\n","            list_labels_input =[]\n","            d_model.trainable = True\n","            for ii in range(5):\n","                # get randomly selected 'real' samples\n","                [X_real, labels_real], [y_real, onehotenc] = generate_real_samples(dataset, n_batch)\n","                # update discriminator model weights\n","                d_loss1, _ = d_model.train_on_batch([X_real, labels_real], y_real)\n","                # generate 'fake' examples\n","                # prepare points in latent space as input for the generator\n","                [z_input, labels_input]  = generate_latent_points(latent_dim, n_batch)\n","                [X_fake, labels], y_fake = generate_fake_samples(g_model, z_input, labels_input, n_batch)\n","                # update discriminator model weights\n","                d_loss2, _ = d_model.train_on_batch([X_fake, labels], y_fake)\n","\n","                list_z_input.append(z_input)\n","                list_labels_input.append(labels_input)\n","\n","                for l in d_model.layers:\n","                    weights = l.get_weights()\n","                    weights = [np.clip(w, -dclip_value, dclip_value) for w in weights]\n","                    l.set_weights(weights)\n","\n","            zinput = np.ndarray((5*n_batch, latent_dim))\n","            labelsinput = np.ndarray((5*n_batch,1,16))\n","            cntr = 0\n","            for ii in range(5):\n","                for jj in range(n_batch):\n","                    zinput[cntr,:] = list_z_input[ii][jj,:]\n","                    labelsinput[ii*n_batch:(ii+1)*n_batch,:] = list_labels_input[ii][:]\n","                    cntr = cntr + 1\n","\n","            # create inverted labels for the fake samples\n","            y_gan = np.ones((5*n_batch, 1))\n","            # update the generator via the discriminator's error\n","            d_model.trainable = False\n","            #g_loss = gan_model.train_on_batch([z_input, labels_input], y_gan)\n","            g_loss = gan_model.train_on_batch([zinput, labelsinput], y_gan)\n","\n","            for l in g_model.layers:\n","                weights = l.get_weights()\n","                weights = [np.clip(w, -gclip_value, gclip_value) for w in weights]\n","                l.set_weights(weights)\n","            # summarize loss on this batch\n","            try:\n","                inceptionScore = InceptionScore(nelem=n_batch)\n","            except:\n","                continue\n","            print('epoch: %d/%d, %d/%d, dloss_real: %.3f, dloss_fake: %.3f, gloss: %.3f, IS: %s' %\\\n","                  (i+1, n_epochs, j+1, bat_per_epo, d_loss1, d_loss2, g_loss, inceptionScore))\n","\n","        ix = np.random.randint(0,half_batch)\n","        #print('generated label : ', labels)\n","        #print(\"***************************\")\n","        #print('real label      : ', labels_real)\n","        X_real_abs = X_real[ix,:,0] + 1j*X_real[ix,:,1]\n","        X_fake_abs = X_fake[ix,:,0] + 1j*X_fake[ix,:,1]\n","\n","        print('\\n')\n","        plt.figure(figsize=(15,6))\n","        plt.plot(np.abs(X_real_abs))\n","        plt.plot(np.abs(X_fake_abs))\n","        plt.show()\n","        print('\\n')\n","\n","    # save the generator model\n","    g_model.save('cgan_generator.hdf5')\n","    d_model.save('cgan_discriminator.hdf5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jN3kHdLXXfCj","scrolled":true},"outputs":[],"source":["# train model\n","train(g_model, d_model, gan_model, dataset, latent_dim, \n","      n_epochs=100, n_batch=256, load_saved_data=True, \n","      dclip_value=0.1, gclip_value=0.5)    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"luQfSTz5iXpr"},"outputs":[],"source":["g_model.load_weights('cgan_generator.hdf5')\n","d_model.load_weights('cgan_discriminator.hdf5')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"YFrNJXrQdsmR"},"outputs":[],"source":["# example of loading the generator model and generating images\n","def generateNewSamples():\n","\tn_samples_perclass = 150\n","\tn_classes          = numberOfClasses\n","\t\n","\t# generate points in latent space as input for the generator\n","\tdef generate_latent_points(latent_dim, n_samples_perclass, n_classes=n_classes):\n","\t\tnumberOfsamples = n_samples_perclass*n_classes\n","\t\t# generate points in the latent space\n","\t\tx_input = randn(latent_dim * numberOfsamples)\n","\t\t# reshape into a batch of inputs for the network\n","\t\tz_input = x_input.reshape(numberOfsamples, latent_dim)\n","\t\t# generate labels\n","\t\tonehot = np.zeros((n_classes*n_samples_perclass,1,16))\n","\t\tlabels = np.ndarray((n_classes*n_samples_perclass,)) \n","\t\tfor ii in range(n_classes):\n","\t\t\tfor jj in range(n_samples_perclass):\n","\t\t\t\tonehot[ii*n_samples_perclass+jj,0,ii] = 1\n","\t\t\t\tlabels[ii*n_samples_perclass+jj] = ii\n","\t\treturn [z_input, labels, onehot]\n","\t\n","\n","\t# load model\n","\tmodel = load_model('cgan_generator.hdf5')\n","\t# generate images\n","\tlatent_points, labels, onehot = generate_latent_points(latent_dim=100, n_samples_perclass=n_samples_perclass)\n","\n","\t# generate images\n","\tX1  = model.predict([latent_points, onehot])\n","\t#X1  = inv_scaling(X1,max,min)\n","\tmygenerateddata = []\n","\tmygenerateddata.append(X1)\n","\tmygenerateddata.append(labels)\n","\tmygenerateddata.append(onehot)\n","\n","\t#print(X1.shape)\n","\tfor ii in range(n_samples_perclass*n_classes):\n","\t\tprint(\"label: \",labels[ii])\n","\t\tplt.plot(X1[ii,:,0,0])\n","\t\tplt.plot(X1[ii,:,1,0])\n","\t\tplt.show()\n","\t\tprint(\"******************\")\n","\n","\treturn mygenerateddata"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"f8hUSPAmDlnB"},"outputs":[],"source":["mygenerateddata = generateNewSamples()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Itr4legrPzA"},"outputs":[],"source":["mygenerateddata_inv = mygenerateddata\n","mygenerateddata_inv[0][:,:,0,0] = scx0.inverse_transform(mygenerateddata[0][:,:,0,0])\n","mygenerateddata_inv[0][:,:,1,0] = scx1.inverse_transform(mygenerateddata[0][:,:,1,0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zwbrpB_wrPzA"},"outputs":[],"source":["import pickle\n","pickle.dump(mygenerateddata_inv,open(\"generated_inv01.pickle\",\"wb\"))\n","mygenerateddata_inv[0].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HxYrVpGyrPzA"},"outputs":[],"source":["plt.plot(mygenerateddata_inv[0][0:100,:,1,0])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"NHhLMMZC5uq8"},"outputs":[],"source":["def mergeDatasets():\n","    print(\"generateddata shape: \",mygenerateddata[0].shape)\n","    print(\"original data shape before merging: \",dataset[0].shape)\n","\n","    mygenerateddata[2] = mygenerateddata[2].squeeze()\n","\n","    list_data_base = [dataset,mygenerateddata]\n","    total_data = len(dataset[0]) + len(mygenerateddata[0])\n","\n","    ## Merging the Generated data set to the existing one\n","    newdataset = np.ndarray((total_data,dataset[0].shape[1],dataset[0].shape[2],dataset[0].shape[3]))\n","    newlabels  = np.ndarray((total_data,))\n","    newonehot  = np.ndarray((total_data,16))\n","\n","    newdataset[0:len(dataset[0])] = dataset[0]\n","    newdataset[len(dataset[0]):] = mygenerateddata[0]\n","\n","    newlabels[0:len(dataset[1])] = dataset[1]\n","    newlabels[len(dataset[1]):] = mygenerateddata[1]\n","\n","    newonehot[0:len(dataset[2])] = dataset[2]\n","    newonehot[len(dataset[2]):] = mygenerateddata[2]\n","\n","    print(\"dataset shape after merging: \",newdataset.shape)\n","    datasetNew = [newdataset,newlabels,newonehot]\n","    return datasetNew\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xDt8zwsudsmS"},"outputs":[],"source":["MergedDataset = mergeDatasets()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WK75yD0xrPzB"},"outputs":[],"source":["list_classLabels,counterTrue,counterFalse=postprocess(classifier,dataset)\n","list_classLabelsGen,counterTrue,counterFalse=postprocess(classifier,mygenerateddata)\n","list_classLabelsMerged,counterTrue,counterFalse=postprocess(classifier,MergedDataset)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mybLXMrlrPzB"},"outputs":[],"source":["print(\"Generated Classes after Conditional GAN training\")\n","plt.scatter(np.arange(len(list_classLabelsGen)),mygenerateddata[1])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"du7UpsQrrPzB"},"outputs":[],"source":["print(\"Generated Classes Determined by the Classifier Trained with Original Dataset\")\n","plt.scatter(np.arange(len(list_classLabelsGen)),list_classLabelsGen)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"mEubcd3yrPzC"},"source":["Sonuçlar incelendiğinde:\n","    - Her sınıftan eşit sayıda sınıf oluşturması gerekirken bunu yapmadığı,\n","    - Conditional GAN'ın 5, 7, ve 8. sınıflardan yeni sınıf oluşturmada zorlandığı,\n","    - oluşturulan sınıfların classifier tarafından sınıflandırıldığında, farklı label'lı sınıf olarak gördüğü\n","anlaşılmakta.\n","\n","Ancak, yine de kontrolsüz de olsa, birden fazla sınıf için yeni veri üretebilmiş gibiyiz. Bunu test etmek için knn vb yöntemleri de kullanarak sınıflandırma yapalım."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jz71h5crrPzJ"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hj6tWf7ndsmW"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Copy of cdcgan_RF.ipynb","private_outputs":true,"provenance":[{"file_id":"1SzdS1Gs_iATaw9ZAUvZH6Lef0qXS_BOv","timestamp":1650436920712}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"nbTranslate":{"displayLangs":["*"],"hotkey":"alt-t","langInMainMenu":true,"sourceLang":"en","targetLang":"fr","useGoogleTranslate":true},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":true}},"nbformat":4,"nbformat_minor":0}