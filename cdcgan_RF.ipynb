{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/ibrahimkaya754/GAN/blob/main/dcgan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"e1_Y75QXJS6h"},"source":["### Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dm3J1J7wsKW3"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VZVgc00ds31l"},"outputs":[],"source":["cd drive/MyDrive/ESEN/Scripts/CGAN_with_Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bH7BE3z29NYI"},"outputs":[],"source":["## MODULES AND THE DATA IMPORT\n","from numpy import expand_dims\n","from numpy import zeros\n","from numpy import ones\n","from numpy.random import randn\n","from numpy.random import randint\n","from keras.datasets.fashion_mnist import load_data\n","from tensorflow.keras.optimizers import *\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.initializers import *\n","import tensorflow as tf\n","import numpy as np\n","import pickle\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import MinMaxScaler\n","from numpy import asarray\n","from numpy.random import randn\n","from numpy.random import randint\n","from keras.models import load_model\n","from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.constraints import Constraint\n","from tensorflow.keras.losses import *\n","#from MoGeNA import *\n","from sklearn.preprocessing import *\n","from tensorflow.python.platform import build_info as tf_build_info\n","from scipy import spatial\n","#from kymatio.numpy import Scattering2D\n","print(tf_build_info.build_info)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gez9Mqf4rPys"},"outputs":[],"source":["gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n","for device in gpu_devices:\n","    tf.config.experimental.set_memory_growth(device, True)\n","    print(\"tensorflow version is: \",tf.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V6eiuGf01S4E"},"outputs":[],"source":["# Load The Data\n","def load_wifi_data(data,label,labelClass=['Samsung Galaxy M21', 'Samsung S21', 'Iphone 7 Plus', 'Iphone 8', 'Iphone 12 Pro Max', 'Oppo Realme 6', 'Xiaomi Mi 9T Pro', 'Iphone X', 'Iphone 11']):\n","    data_wifi   = pickle.load(open(data,\"rb\"))\n","    labels_wifi = pickle.load(open(label,\"rb\"))\n","    \n","    labels  = []\n","    labels_wifi_numeric = np.ndarray(shape=labels_wifi.shape,dtype=np.int16)\n","\n","#     data_I = np.ndarray((data_wifi.shape[0],int(data_wifi.shape[1]/2),data_wifi.shape[2]),dtype=np.float64)\n","#     data_Q = np.ndarray((data_wifi.shape[0],int(data_wifi.shape[1]/2),data_wifi.shape[2]),dtype=np.float64)\n","\n","#     data_I[:,0::1,0] = data_wifi[:,0::2,0]\n","#     data_Q[:,0::1,0] = data_wifi[:,1::2,0]\n","\n","#     data_wifi_reshaped = np.ndarray((data_wifi.shape),dtype=np.float64)\n","#     data_wifi_reshaped = data_wifi_reshaped.reshape(data_wifi_reshaped.shape[0],1024,2)\n","#     data_wifi_reshaped[:,:,0]= data_I[:,:,0]\n","#     data_wifi_reshaped[:,:,1]= data_Q[:,:,0]\n","    \n","    #scattering = Scattering2D(J=1, shape=(1024, 2))\n","    #data_wifi = scattering(data_wifi)\n","    \n","    print(\"data_shape:\" ,data_wifi.shape)  \n","    print('label shape: ',labels_wifi.shape)\n","\n","    for lbl in labels_wifi:\n","        if lbl not in labels:\n","            labels.append(lbl)\n","\n","    #labelsnumeric = [value for value,key in enumerate(labelClass)]\n","\n","    for val,label in enumerate(labels_wifi):\n","        labels_wifi_numeric[val] = labelClass.index(label)\n","\n","    print(\"labels: \",labels)\n","    data = {'data'   :data_wifi,\n","            'labels' :labels_wifi,\n","            'labelsNum' :labels_wifi_numeric}\n","\n","    dumbarray = np.zeros((data['labelsNum'].shape[0],16),dtype=np.int16)\n","    for ii in range(data['labelsNum'].shape[0]):\n","        indice = labels_wifi_numeric[ii]\n","        dumbarray[ii,indice] = 1\n","\n","    data['label1hot'] = dumbarray\n","    numberOfClasses = len(labels)\n","    print(\"number of classes: \",numberOfClasses)\n","    return data, numberOfClasses"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0QTZxbZlrPyu"},"outputs":[],"source":["data,numberOfClasses = load_wifi_data(data=\"../daha_yeni_Data/train_data\",label=\"../daha_yeni_Data/train_label\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iYOVrVh8rPyv"},"outputs":[],"source":["data_val,numberOfClassesVal = load_wifi_data(data=\"../daha_yeni_Data/test_data\",label=\"../daha_yeni_Data/test_label\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d2wbPhhhrPyw"},"outputs":[],"source":["#pickle.dump(data,open(\"data_wavelettransformed.pickle\",\"wb\"))\n","#pickle.dump(data_val,open(\"data_val_wavelettransformed.pickle\",\"wb\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C8b9PReYYLJy"},"outputs":[],"source":["# Creating the DataSets - Train and Validation\n","def CreateDataSet(data):\n","    dataset = []\n","    dataset.append(data['data'].reshape(data['data'].shape[0],data['data'].shape[1],data['data'].shape[2],1))\n","    dataset.append(data['labelsNum'])\n","    dataset.append(data['label1hot'])\n","    print('dataset shape: ', dataset[0].shape)\n","    print('max value before scaling: ', np.max(dataset[0]))\n","    print('min value before scaling: ', np.min(dataset[0]))\n","    print(dataset[2].shape)\n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oz1BEgXRrPyx"},"outputs":[],"source":["dataset = CreateDataSet(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nmcOQq0WrPyx"},"outputs":[],"source":["datasetVal= CreateDataSet(data_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zyu1i01WrPyy"},"outputs":[],"source":["def scaling(TrainSet,ValidationSet):\n","    scalerx0 = MinMaxScaler((-0.5,0.5))\n","    scalerx1 = MinMaxScaler((-0.5,0.5))\n","\n","    scx0 = scalerx0.fit(TrainSet[0][:,:,0,0])\n","    scx1 = scalerx1.fit(TrainSet[0][:,:,1,0])\n","\n","    TrainSet[0][:,:,0,0]= scx0.transform(TrainSet[0][:,:,0,0])\n","    TrainSet[0][:,:,1,0]= scx1.transform(TrainSet[0][:,:,1,0])\n","    ValidationSet[0][:,:,0,0]= scx0.transform(ValidationSet[0][:,:,0,0])\n","    ValidationSet[0][:,:,1,0]= scx1.transform(ValidationSet[0][:,:,1,0])\n","    print('min, max value of I after scaling training dataset: %.3f, %.3f' % (np.min(TrainSet[0][:,:,0,0]),np.max(TrainSet[0][:,:,0,0])))\n","    print('min, max value of Q after scaling training dataset: %.3f, %.3f' % (np.min(TrainSet[0][:,:,1,0]),np.max(TrainSet[0][:,:,1,0])))\n","    print('min, max value of I after scaling validation set: %.3f, %.3f' % (np.min(ValidationSet[0][:,:,0,0]),np.max(ValidationSet[0][:,:,0,0])))\n","    print('min, max value of Q after scaling validation set: %.3f, %.3f' % (np.min(ValidationSet[0][:,:,1,0]),np.max(ValidationSet[0][:,:,1,0])))\n","    return TrainSet,ValidationSet,[scx0,scx1]\n","dataset,datasetVal,[scx0,scx1] = scaling(dataset,datasetVal)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f68FPs23_pQI","scrolled":true},"outputs":[],"source":["# Defining the Classifier\n","def define_classifier(in_shape=(1024,2,1), n_classes=numberOfClasses, \n","                      filters=[64,32,16,8], neurons=50,\n","                      strides=[(2,1),(2,2),(2,2),(2,2)],\n","                      alpha= 0.2, kernelSize= 3, Drop= 0.2,\n","                      learningRate= 0.001):\n","\n","    opt      = Adam(lr=learningRate)\n","    in_image = Input(shape=in_shape)\n","\n","    layer1 = Reshape((64,32,1))(in_image)\n","    \n","    for fltr,strd in zip(filters,strides):\n","        layer1 = Conv2D(int(round(fltr)), (int(round(kernelSize)),int(round(kernelSize))), \n","                        strides=strd, padding='same')(layer1)\n","        layer1 = LeakyReLU(alpha=alpha)(layer1)\n","\n","    flatten = Flatten()(layer1)\n","\n","    layer1 = Dropout(Drop)(flatten)\n","    featuredescriptor = Dense(int(round(neurons)))(layer1)\n","    layer1 = LeakyReLU(alpha=alpha)(featuredescriptor)\n","    layer1 = Dropout(Drop)(layer1)\n","\n","    out    = Dense(16,activation='softmax')(layer1)\n","\n","    classifier  = Model(in_image, out)\n","    classifier.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n","    classifier.summary()\n","\n","    descriptor1 = Model(in_image,featuredescriptor)\n","    descriptor2 = Model(in_image,featuredescriptor)\n","    return classifier, descriptor1, descriptor2\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eYOPoT35rPyy"},"outputs":[],"source":["# CALLBACKS\n","def Callbacks():\n","    checkpoint = ModelCheckpoint(filepath=\"./classifier.hdf5\", \n","                                 monitor='val_accuracy', verbose=1, \n","                                 save_best_only=True, period=1, \n","                                 mode='max',save_weights_only=False)\n","    reduce_lr  = ReduceLROnPlateau(monitor='val_accuracy', \n","                                   factor=0.98,\n","                                   patience=10, \n","                                   min_lr=0.0000001, mode='max', verbose=1)\n","    early_stopping = EarlyStopping(monitor='val_accuracy', patience=250, verbose=1, mode='auto') \n","    return checkpoint, reduce_lr, early_stopping\n","checkpoint, reduce_lr, early_stopping = Callbacks()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"asQqcGO3rPyz"},"outputs":[],"source":["# Function for MoGenA\n","def optClassifier(inp):\n","    print(\"parameter set is: \",inp)\n","    filters      = inp[0:4]\n","    neurons      = inp[4]\n","    alpha        = inp[5]\n","    kernelSize   = inp[6]\n","    Drop         = inp[7]\n","    learningRate = inp[8]\n","    classifier   = define_classifier(in_shape=(9,512,1), n_classes=14, \n","                                     filters=filters, neurons=neurons,\n","                                     strides=[(2,1),(2,2),(2,2),(2,2)],\n","                                     alpha= alpha, kernelSize= kernelSize, Drop= Drop,\n","                                     learningRate= learningRate)\n","    history      = classifier.fit(dataset[0], dataset[2], \n","                                  batch_size=int(round(inp[9])), \n","                                  epochs=int(round(inp[10])), \n","                                  validation_data=(datasetVal[0],datasetVal[2]), \n","                                  #validation_split = 0.25,\n","                                  verbose=1, shuffle=True,\n","                                  callbacks=[checkpoint, reduce_lr])\n","    minimization_param = -np.max(history.history['val_accuracy'])\n","    return minimization_param"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5H8FKK1RrPyz","scrolled":true},"outputs":[],"source":["# Genetic Algorithm\n","runOpt = False\n","if runOpt:\n","  ga = Mogena.MoGenA(fitness_func         = optClassifier, \n","                    number_of_generation = 20,\n","                    number_of_variables  = 11,\n","                    population_size      = 30,\n","                    lower_boundaries     = [8  ,8  ,8  ,8  ,8  ,0.1,3,0.1,0.00001,64  ,20 ],\n","                    upper_boundaries     = [250,250,250,250,250,0.5,7,0.5,0.001  ,1024,200],\n","                    use_saved_data       = False)\n","  result =  {'best_fitness_ever'            : ga.best_fitness_ever,\n","            'best_individual_decoded_ever' : ga.best_individual_decoded_ever}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z1Lh5Ja9rPy4"},"outputs":[],"source":["#result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XFiX3DPArPy5"},"outputs":[],"source":["# PostProcessing Of GA\n","optParams= [\"filter0\",\"filter1\",\"filter2\",\"filter3\",\"DenseNeurons\",\"LeakyReluAlpha\",\"kernelSize\",\"DropOutRatio\",\n","            \"LearningRate\",\"batchSize\",\"Epochs\"]\n","bestEver= [int(round(246.83139228339436)),\n","           int(round(154.999)),\n","           int(round(75.258)),\n","           int(round(213.6516)),\n","           int(round(183.0685)),\n","           0.141,\n","           int(round(6.666)),\n","           0.205,\n","           0.000455,\n","           int(round(191.8847)),\n","           int(round(30))]\n","print(bestEver)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Al33DI9trPy5"},"outputs":[],"source":["# Reloading the Optimized Classifier to BackTest\n","def ClassifierOptimized(inp,loadweights=True):\n","    print(\"parameter set is: \",inp)\n","    filters      = inp[0:4]\n","    neurons      = inp[4]\n","    alpha        = inp[5]\n","    kernelSize   = inp[6]\n","    Drop         = inp[7]\n","    learningRate = inp[8]\n","    classifier,d1,d2   = define_classifier(in_shape=(1024,2,1), n_classes=14, \n","                                     filters=filters, neurons=neurons,\n","                                     strides=[(2,1),(2,2),(2,2),(2,2)],\n","                                     alpha= alpha, kernelSize= kernelSize, Drop= Drop,\n","                                     learningRate= learningRate)\n","    tf.keras.utils.plot_model(classifier,show_shapes=True)\n","    if loadweights:\n","      classifier.load_weights(\"./classifier.hdf5\")\n","    history      = classifier.fit(dataset[0], dataset[2], \n","                                  batch_size=int(round(inp[9])), \n","                                  epochs=int(round(inp[10])), \n","                                  validation_data=(datasetVal[0],datasetVal[2]), \n","                                  verbose=1, shuffle=True,\n","                                  callbacks=[checkpoint, reduce_lr])\n","    return classifier,history,d1,d2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GPL8tDrqrPy5","scrolled":true},"outputs":[],"source":["# Generate Classifier\n","loadWeights = False\n","classifier,history,d1,d2= ClassifierOptimized(bestEver,loadWeights)"]},{"cell_type":"markdown","source":["Below is the section to find Cosine Similarity (CS) of the Classes\n","The CS is calculated for every class pairs, e.g. 0-0, 0,1, 0,2, etc.\n","\n","dict_mean_cos is the dictionary keeping all the values for CS."],"metadata":{"id":"NUqCVIqeeJF9"}},{"cell_type":"code","source":["## We define CS4OriginalData function to make the calculation for CS for all class pairs.\n","## The dictionary \"dict_mean_cos\" is created inside this function.\n","def CS4OriginalData():\n","  nOfClass= dataset[2].shape[1]\n","  dict_mean_cos = {str(ii): {\"indices\":[]} for ii in range(nOfClass)}\n","  total_DataNumber = 0\n","\n","  for key in dict_mean_cos.keys():\n","    for ii in range(len(dataset[1])):\n","      if dataset[1][ii] == int(key):\n","        dict_mean_cos[key][\"indices\"].append(ii)\n","\n","  for key in dict_mean_cos.keys():\n","    dict_mean_cos[key][\"shape\"] = dataset[1][dict_mean_cos[key][\"indices\"]].shape[0]\n","    print(\"number of data existing for each class %s: %d\" % (key,dict_mean_cos[key][\"shape\"]))\n","    total_DataNumber = total_DataNumber + dict_mean_cos[key][\"shape\"]\n","  \n","  print(\"total_data_number: %d\" % (total_DataNumber))\n","  print(\"dataset shape is: \", dataset[1].shape[0])\n","  print(\"--------------------------------------------\")\n","  for key1 in dict_mean_cos.keys():\n","    for key2 in dict_mean_cos.keys():\n","      try:\n","        #print(\"the cosine similarity is calculated for %s and %s \" % (key1,key2))\n","        res1 = d1.predict(dataset[0][dict_mean_cos[key1][\"indices\"]])\n","        res2 = d2.predict(dataset[0][dict_mean_cos[key2][\"indices\"]])\n","        list_cs = []\n","        for ii in range(50):\n","          ind1, ind2 = np.random.randint(0,1000,2)\n","          cosinesimilarity = 1- spatial.distance.cosine(res1[ind1], res2[ind2])\n","          #print(cosinesimilarity)\n","          list_cs.append(cosinesimilarity)\n","        string = \"mean cosine similarity for %s and %s\" % (key1,key2)\n","        print(\"%s is: %.3f\" % (string,np.mean(list_cs)))\n","        dict_mean_cos[key1][string] = np.mean(list_cs)\n","      except:\n","        continue\n","    print(\"********************************************\")\n","  return dict_mean_cos\n","dict_mean_cos = CS4OriginalData()"],"metadata":{"id":"bxhM7Er7VTkA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Below is the section where we load the synthetically generated data and compare it with the original one using cosine similarity."],"metadata":{"id":"bu6Yzw9dfcIf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jw-e7ih6xzcA"},"outputs":[],"source":["def load_synthetic_data(data_location):\n","  synthetic_data   = pickle.load(open(data_location,\"rb\"))\n","  synthetic_data[:,:,0]= scx0.transform(synthetic_data[:,:,0])\n","  synthetic_data[:,:,1]= scx1.transform(synthetic_data[:,:,1])\n","  print(\"Synthetic Data Shape is: \", synthetic_data.shape)\n","  synthetic_data_result = classifier.predict(synthetic_data)\n","  listClassifiedSyntheticData = [np.argmax(synthetic_data_result[ii]) for ii in range(len(synthetic_data))]\n","  print(\"Generated Classes are: \", listClassifiedSyntheticData)\n","  return synthetic_data, listClassifiedSyntheticData\n","synthetic_data, listClassifiedSyntheticData= load_synthetic_data(\"../mygenerateddata_normal2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XJ07D_Cu0Yoo"},"outputs":[],"source":["## CompareCS4SynData is to compare the cosine similarity for the synthetic (syn) data and the original data.\n","## We compare the synthetic data classes determined by the classifier with the original classes existing in the original dataset\n","def CompareCS4SynData(synthetic_data,listClassifiedSyntheticData):\n","  res2 = d2.predict(synthetic_data)\n","  for key,value in enumerate(listClassifiedSyntheticData):\n","    res1 = d1.predict(dataset[0][dict_mean_cos[str(value)][\"indices\"]])\n","    list_cs = []\n","    for ii in range(50):\n","      ind1 = np.random.randint(0,1000,1)\n","      cosinesimilarity = 1- spatial.distance.cosine(res1[ind1], res2[key])\n","      #print(cosinesimilarity)\n","      list_cs.append(cosinesimilarity)\n","    string = \"mean cosine similarity for class %s and 'newly generated class %s'\" % (value,value)\n","    print(\"%s is: %.3f\" % (string,np.mean(list_cs)))\n","    #dict_mean_cos[key1][string] = np.mean(list_cs)\n","CompareCS4SynData(synthetic_data,listClassifiedSyntheticData)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G2tj49Kk0Yqo"},"outputs":[],"source":["plt.scatter(listClassifiedSyntheticData,np.arange(len(listClassifiedSyntheticData)))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fwPLLW8-_pSe","scrolled":true},"outputs":[],"source":["tf.keras.utils.plot_model(classifier,show_shapes=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6YmXmyZGBFAq"},"outputs":[],"source":["# Postprocess\n","def postprocess(model,dataset):\n","    classifier.load_weights(\"./classifier.hdf5\")\n","    results = model.predict(dataset[0])\n","    list_class = []\n","    for ii in range(results.shape[0]):\n","        list_class.append(np.argmax(results[ii]))\n","\n","    counterTrue  = 0\n","    counterFalse = 0\n","    for ii in range(len(list_class)):\n","        if list_class[ii] == dataset[1][ii]:\n","            counterTrue = counterTrue + 1\n","        else:\n","            counterFalse = counterFalse + 1\n","    print(\"True:%d    False:%d\" % (counterTrue,counterFalse))\n","    return list_class,counterTrue,counterFalse"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P8u-DKqKGz0E"},"outputs":[],"source":["list_class,counterTrue,counterFalse=postprocess(classifier,datasetVal)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zsw0bU2hAX90"},"outputs":[],"source":["## GENERATIVE MODEL (CONDITIONAL GAN)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nkMeyqNcAYAs"},"outputs":[],"source":["class Spectral_Norm(Constraint):\n","    def __init__(self, power_iters=5):\n","        self.n_iters = power_iters\n","        \n","    def l2_normalize(self, x, eps=1e-12):\n","        return x / tf.linalg.norm(x + eps)\n","\n","    def __call__(self, w):\n","        flattened_w = tf.reshape(w, [w.shape[0], -1])\n","        u = tf.random.normal([flattened_w.shape[0]])\n","        v = tf.random.normal([flattened_w.shape[1]])\n","        for i in range(self.n_iters):\n","            v = tf.linalg.matvec(tf.transpose(flattened_w), u)\n","            v = self.l2_normalize(v)\n","            u = tf.linalg.matvec(flattened_w, v)\n","            u = self.l2_normalize(u)\n","        sigma = tf.tensordot(u, tf.linalg.matvec(flattened_w, v), axes=1)\n","        return w / sigma\n","\n","    def get_config(self):\n","        return {'n_iters': self.n_iters}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jB-VZfl4rPy7"},"outputs":[],"source":["# optParams= [\"filter0\",\"filter1\",\"filter2\",\"filter3\",\"DenseNeurons\",\"LeakyReluAlpha\",\"kernelSize\",\"DropOutRatio\",\n","#             \"LearningRate\",\"batchSize\",\"Epochs\"]\n","# bestEver= [int(round(246.83139228339436)),\n","#            int(round(154.999)),\n","#            int(round(75.258)),\n","#            int(round(213.6516)),\n","#            int(round(183.0685)),\n","#            0.141,\n","#            int(round(6.666)),\n","#            0.205,\n","#            0.000455,\n","#            int(round(191.8847)),\n","#            int(round(30))]\n","# print(bestEver)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VFnJtELM9nna"},"outputs":[],"source":["# define the standalone discriminator model\n","def define_discriminator(in_shape=(1024,2,1), n_classes=numberOfClasses, convDim='2D'):\n","    # label input\n","    initializer = tf.keras.initializers.GlorotNormal()\n","    const = Spectral_Norm()\n","    in_label = Input(shape=(1,16))\n","    # embedding for categorical input\n","    li = Embedding(16, 50)(in_label)\n","    # scale up to image dimensions with linear activation\n","    n_nodes = in_shape[0] * in_shape[1]\n","    li = Dense(n_nodes)(li)\n","    if convDim == '1D':\n","        # reshape to additional channel\n","        li = Reshape((in_shape[0], in_shape[1]))(li)\n","        # image input\n","        in_image = Input(shape=in_shape[0:2])\n","        # concat label as a channel\n","        merge = Concatenate()([in_image, li])\n","        merge = Reshape((64,64))(merge)\n","        # downsample\n","        layer1 = Conv1D(128, (3), strides=(2), padding='same',\n","                        kernel_initializer = glorot_normal(),\n","                        kernel_constraint= const)(merge)\n","        layer1 = LeakyReLU(alpha=0.2)(layer1)\n","\n","        layer1 = Conv1D(64, (3), strides=(2), padding='same',\n","                        kernel_initializer = glorot_normal(),\n","                        kernel_constraint= const)(layer1)\n","        layer1 = LeakyReLU(alpha=0.2)(layer1)\n","\n","        layer1 = Conv1D(32, (3), strides=(2), padding='same',\n","                        kernel_initializer = glorot_normal(),\n","                        kernel_constraint= const)(layer1)\n","        layer1 = LeakyReLU(alpha=0.2)(layer1)\n","\n","    else:\n","        leakyReluAlpha = 0.2\n","        ksize = (3,3)\n","        # reshape to additional channel\n","        li = Reshape((in_shape[0], in_shape[1], 16))(li)\n","        # image input\n","        in_image = Input(shape=in_shape)\n","        # concat label as a channel\n","        merge = Concatenate()([in_image, li])\n","        merge = Reshape((64,32,17))(merge)\n","        # downsample\n","        layer1 = Conv2D(128, ksize, strides=(2,1), padding='same',\n","                        kernel_initializer = initializer,\n","                        kernel_constraint= const)(merge)\n","        layer1 = LeakyReLU(alpha=leakyReluAlpha)(layer1)\n","\n","        layer1 = Conv2D(64, ksize, strides=(2,2), padding='same',\n","                        kernel_initializer = initializer,\n","                        kernel_constraint= const)(layer1)\n","        layer1 = LeakyReLU(alpha=leakyReluAlpha)(layer1)\n","\n","        layer1 = Conv2D(32, ksize, strides=(2,2), padding='same',\n","                        kernel_initializer = initializer,\n","                        kernel_constraint= const)(layer1)\n","        layer1 = LeakyReLU(alpha=leakyReluAlpha)(layer1)\n","        \n","        layer1 = Conv2D(16, ksize, strides=(2,2), padding='same',\n","                        kernel_initializer = initializer,\n","                        kernel_constraint= const)(layer1)\n","        layer1 = LeakyReLU(alpha=leakyReluAlpha)(layer1)\n","\n","    layer1 = Flatten()(layer1)\n","    layer1 = Dropout(0.2)(layer1)\n","    # output\n","    out_layer = Dense(1, activation='sigmoid')(layer1)\n","    # define model\n","    model = Model([in_image, in_label], out_layer)\n","    # compile model\n","    opt = RMSprop(lr=0.0001)\n","    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","    model.summary()\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gcZKpierWzHk"},"outputs":[],"source":["# size of the latent space\n","latent_dim = 100"]},{"cell_type":"code","source":["numberOfClasses"],"metadata":{"id":"UQhfC7Rb-q24"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"orYWSZLT9toc"},"outputs":[],"source":["# define the standalone generator model\n","def define_generator(latent_dim, n_classes=numberOfClasses, convDim='2D'):\n","    # label input\n","    initializer = tf.keras.initializers.GlorotNormal()\n","    in_label = Input(shape=(1,16))\n","    # embedding for categorical input\n","    li = Embedding(16, 50)(in_label)\n","    # linear multiplication\n","  \n","    if convDim== '1D':\n","        n_nodes = 8 * 16 \n","        li = Dense(n_nodes)(li)\n","        # reshape to additional channel\n","        li = Reshape((64, 2))(li)\n","        # image generator input\n","        in_lat = Input(shape=(latent_dim,))\n","        # foundation for 7x7 image\n","        n_nodes = 64 * 64\n","        gen = Dense(n_nodes)(in_lat)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","        gen = Reshape((64, 64))(gen)\n","        # merge image gen and label input\n","        merge = Concatenate()([gen, li])\n","          # upsample to 14x14\n","        gen = Conv1DTranspose(16, (4), strides=(4), padding='same',kernel_initializer = glorot_normal())(merge)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","        # upsample to 28x28\n","        gen = Conv1DTranspose(16, (4), strides=(4), padding='same',kernel_initializer = glorot_normal())(gen)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","\n","        gen = Conv1DTranspose(16, (4), strides=(2), padding='same',kernel_initializer = glorot_normal())(gen)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","        # output\n","        gen = Conv1D(1, (4), activation='tanh', padding='same',kernel_initializer = glorot_normal())(gen)\n","        out_layer = Reshape((1024, 2))(gen)\n","    else:\n","        n_nodes = 4 * 4\n","        li = Dense(n_nodes)(li)\n","        # reshape to additional channel\n","        li = Reshape((16, 4*4, 1))(li)\n","        # image generator input\n","        in_lat = Input(shape=(latent_dim,))\n","        # foundation for 7x7 image\n","        n_nodes = 4 * 4 * 16\n","        gen = Dense(n_nodes)(in_lat)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","        gen = Reshape((16, 4*4, 1))(gen)\n","        # merge image gen and label input\n","        merge = Concatenate()([gen, li])\n","\n","        gen = Conv2DTranspose(256, (4,4), strides=(2,2), padding='same',kernel_initializer = initializer)(merge)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","\n","        gen = Conv2DTranspose(128, (4,4), strides=(2,1), padding='same',kernel_initializer = initializer)(gen)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","\n","        gen = Conv2DTranspose(128, (4,4), strides=(1,1), padding='same',kernel_initializer = initializer)(gen)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","\n","        gen = Conv2DTranspose(64, (4,4), strides=(1,1), padding='same',kernel_initializer = initializer)(gen)\n","        gen = LeakyReLU(alpha=0.2)(gen)\n","    \n","        gen = Conv2D(1, (3,3), strides=(1,1), activation='tanh', padding='same',\n","                     kernel_initializer = initializer)(gen)\n","        gen = Dropout(0.2)(gen)\n","    out_layer = Reshape((1024, 2, 1))(gen)\n","    # define model\n","    model = Model([in_lat, in_label], out_layer)\n","    model.summary()\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UD1E8McUoppi"},"outputs":[],"source":["# Inception Score for measuring the similarity between the generated signals and the real ones, using KL Divergence\n","def InceptionScore(nelem=100):\n","    for tt in range(1):\n","        RealImageList = []\n","        latent_points, labels = generate_latent_points(latent_dim=100, n_samples=30)\n","        for elem in range(labels.shape[0]):\n","            nolabel = np.argmax(labels[elem])\n","            for ii in range(1000):\n","                if dataset[1][ii] == nolabel:\n","                    indice = ii\n","        \n","            RealImageList.append(dataset[0][indice])\n","\n","        RealImageList= np.array(RealImageList)\n","        X1  = g_model.predict([latent_points, labels])\n","        kl  = KLDivergence()\n","        bce = BinaryCrossentropy(from_logits=True)\n","\n","    return kl(RealImageList,X1).numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v6oBxiZS9wrY"},"outputs":[],"source":["# define the combined generator and discriminator model, for updating the generator\n","def define_gan(g_model, d_model):\n","    # make weights in the discriminator not trainable\n","    #d_model.trainable = False\n","\n","    in_label = Input(shape=(1,16))\n","    in_lat = Input(shape=(latent_dim,))\n","\n","    gen_out = g_model([in_lat,in_label])\n","\n","    gan_out = d_model([gen_out,in_label])\n","\n","    # get noise and label inputs from generator model\n","    #gen_noise, gen_label = g_model.input\n","    # get image output from the generator model\n","    #gen_output = g_model.output\n","    # connect image output and label input from generator as inputs to discriminator\n","    #gan_output = d_model([gen_output, gen_label])\n","\n","    # define gan model as taking noise and label and outputting a classification\n","    model = Model([in_lat,in_label], gan_out)\n","    # compile model\n","    opt = RMSprop(lr=0.0003)\n","    model.compile(loss='binary_crossentropy', optimizer=opt)\n","    #model.compile(loss=Ganloss, optimizer=opt)\n","    model.summary()\n","\n","    return model"]},{"cell_type":"code","source":["dataset[2].shape"],"metadata":{"id":"O9RgHPpo-3NT"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rv02rxjoXSCa"},"outputs":[],"source":["# select real samples\n","def generate_real_samples(dataset, n_samples):\n","\t# split into images and labels\n","\timages, labels, onehotenc = dataset\n","\t# choose random instances\n","\tix = randint(0, images.shape[0], n_samples)\n","\t# select images and labels\n","\tX, labels, onehotenc = images[ix], labels[ix], onehotenc[ix]\n","\tonehotenc = onehotenc.reshape((n_samples,1,16))\n","\t# generate class labels\n","\ty = np.ones((n_samples, 1))\n","\treturn [X, onehotenc], [y, onehotenc]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"trF69d8AXWDi"},"outputs":[],"source":["# generate points in latent space as input for the generator\n","def generate_latent_points(latent_dim, n_samples, n_classes=numberOfClasses):\n","\t# generate points in the latent space\n","\tx_input = randn(latent_dim * n_samples)\n","\t# reshape into a batch of inputs for the network\n","\tz_input = x_input.reshape(n_samples, latent_dim)\n","\t# generate labels\n","\tlabels = randint(0, n_classes, n_samples)\n","\tdumbarray = np.zeros((n_samples,1,16))\n","\tfor ii in range(n_samples):\n","\t\tindice = labels[ii]\n","\t\tdumbarray[ii,0,indice] = 1\n","\treturn [z_input, dumbarray]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kUKaHkbvXZrr"},"outputs":[],"source":["# use the generator to generate n fake examples, with class labels\n","def generate_fake_samples(generator, z_input, labels_input, n_samples):\n","\t# predict outputs\n","\timages = generator.predict([z_input, labels_input])\n","\t# create class labels\n","\ty = np.zeros((n_samples, 1))\n","\treturn [images, labels_input], y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x3ATFLSGrPy-","scrolled":true},"outputs":[],"source":["# create the discriminator\n","d_model = define_discriminator(convDim='2D')\n","tf.keras.utils.plot_model(d_model, show_shapes=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZsL6GlPuV_Dw"},"outputs":[],"source":["# create the generator\n","g_model = define_generator(latent_dim,convDim='2D')\n","tf.keras.utils.plot_model(g_model, show_shapes=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RCofZKnlXHI_","scrolled":true},"outputs":[],"source":["# create the gan\n","gan_model = define_gan(g_model, d_model)\n","tf.keras.utils.plot_model(gan_model,show_shapes=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"12Em1-YedsmR"},"outputs":[],"source":["# train the generator and discriminator\n","def train(g_model, d_model, gan_model, dataset, latent_dim, \n","          n_epochs=100, n_batch=128, load_saved_data=True, \n","          dclip_value=0.1, gclip_value=0.1):\n","    \n","    if load_saved_data:\n","        g_model.load_weights('cgan_generator.hdf5')\n","        d_model.load_weights('cgan_discriminator.hdf5')\n","  \n","    bat_per_epo = int(dataset[0].shape[0] / n_batch)\n","    half_batch = int(n_batch / 2)\n","    # manually enumerate epochs\n","    for i in range(n_epochs):\n","        if i%10 == 0:\n","          # save the generator model\n","            g_model.save('cgan_generator.hdf5')\n","            d_model.save('cgan_discriminator.hdf5')\n","        # enumerate batches over the training set\n","        for j in range(bat_per_epo):\n","            list_z_input = []\n","            list_labels_input =[]\n","            d_model.trainable = True\n","            for ii in range(5):\n","                # get randomly selected 'real' samples\n","                [X_real, labels_real], [y_real, onehotenc] = generate_real_samples(dataset, n_batch)\n","                # update discriminator model weights\n","                d_loss1, _ = d_model.train_on_batch([X_real, labels_real], y_real)\n","                # generate 'fake' examples\n","                # prepare points in latent space as input for the generator\n","                [z_input, labels_input]  = generate_latent_points(latent_dim, n_batch)\n","                [X_fake, labels], y_fake = generate_fake_samples(g_model, z_input, labels_input, n_batch)\n","                # update discriminator model weights\n","                d_loss2, _ = d_model.train_on_batch([X_fake, labels], y_fake)\n","\n","                list_z_input.append(z_input)\n","                list_labels_input.append(labels_input)\n","\n","                for l in d_model.layers:\n","                    weights = l.get_weights()\n","                    weights = [np.clip(w, -dclip_value, dclip_value) for w in weights]\n","                    l.set_weights(weights)\n","\n","            zinput = np.ndarray((5*n_batch, latent_dim))\n","            labelsinput = np.ndarray((5*n_batch,1,16))\n","            cntr = 0\n","            for ii in range(5):\n","                for jj in range(n_batch):\n","                    zinput[cntr,:] = list_z_input[ii][jj,:]\n","                    labelsinput[ii*n_batch:(ii+1)*n_batch,:] = list_labels_input[ii][:]\n","                    cntr = cntr + 1\n","\n","            # create inverted labels for the fake samples\n","            y_gan = np.ones((5*n_batch, 1))\n","            # update the generator via the discriminator's error\n","            d_model.trainable = False\n","            #g_loss = gan_model.train_on_batch([z_input, labels_input], y_gan)\n","            g_loss = gan_model.train_on_batch([zinput, labelsinput], y_gan)\n","\n","            for l in g_model.layers:\n","                weights = l.get_weights()\n","                weights = [np.clip(w, -gclip_value, gclip_value) for w in weights]\n","                l.set_weights(weights)\n","            # summarize loss on this batch\n","            try:\n","                inceptionScore = InceptionScore(nelem=n_batch)\n","            except:\n","                continue\n","            print('epoch: %d/%d, %d/%d, dloss_real: %.3f, dloss_fake: %.3f, gloss: %.3f, IS: %s' %\\\n","                  (i+1, n_epochs, j+1, bat_per_epo, d_loss1, d_loss2, g_loss, inceptionScore))\n","\n","        ix = np.random.randint(0,half_batch)\n","        #print('generated label : ', labels)\n","        #print(\"***************************\")\n","        #print('real label      : ', labels_real)\n","        X_real_abs = X_real[ix,:,0] + 1j*X_real[ix,:,1]\n","        X_fake_abs = X_fake[ix,:,0] + 1j*X_fake[ix,:,1]\n","\n","        print('\\n')\n","        plt.figure(figsize=(15,6))\n","        plt.plot(np.abs(X_real_abs))\n","        plt.plot(np.abs(X_fake_abs))\n","        plt.show()\n","        print('\\n')\n","\n","    # save the generator model\n","    g_model.save('cgan_generator.hdf5')\n","    d_model.save('cgan_discriminator.hdf5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jN3kHdLXXfCj","scrolled":true},"outputs":[],"source":["# train model\n","train(g_model, d_model, gan_model, dataset, latent_dim, \n","      n_epochs=100, n_batch=256, load_saved_data=False, \n","      dclip_value=0.1, gclip_value=0.5)    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"luQfSTz5iXpr"},"outputs":[],"source":["g_model.load_weights('cgan_generator.hdf5')\n","d_model.load_weights('cgan_discriminator.hdf5')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"YFrNJXrQdsmR"},"outputs":[],"source":["# example of loading the generator model and generating images\n","def generateNewSamples():\n","\tn_samples_perclass = 150\n","\tn_classes          = numberOfClasses\n","\t\n","\t# generate points in latent space as input for the generator\n","\tdef generate_latent_points(latent_dim, n_samples_perclass, n_classes=n_classes):\n","\t\tnumberOfsamples = n_samples_perclass*n_classes\n","\t\t# generate points in the latent space\n","\t\tx_input = randn(latent_dim * numberOfsamples)\n","\t\t# reshape into a batch of inputs for the network\n","\t\tz_input = x_input.reshape(numberOfsamples, latent_dim)\n","\t\t# generate labels\n","\t\tonehot = np.zeros((n_classes*n_samples_perclass,1,16))\n","\t\tlabels = np.ndarray((n_classes*n_samples_perclass,)) \n","\t\tfor ii in range(n_classes):\n","\t\t\tfor jj in range(n_samples_perclass):\n","\t\t\t\tonehot[ii*n_samples_perclass+jj,0,ii] = 1\n","\t\t\t\tlabels[ii*n_samples_perclass+jj] = ii\n","\t\treturn [z_input, labels, onehot]\n","\t\n","\n","\t# load model\n","\tmodel = load_model('cgan_generator.hdf5')\n","\t# generate images\n","\tlatent_points, labels, onehot = generate_latent_points(latent_dim=100, n_samples_perclass=n_samples_perclass)\n","\n","\t# generate images\n","\tX1  = model.predict([latent_points, onehot])\n","\t#X1  = inv_scaling(X1,max,min)\n","\tmygenerateddata = []\n","\tmygenerateddata.append(X1)\n","\tmygenerateddata.append(labels)\n","\tmygenerateddata.append(onehot)\n","\n","\t#print(X1.shape)\n","\tfor ii in range(n_samples_perclass*n_classes):\n","\t\tprint(\"label: \",labels[ii])\n","\t\tplt.plot(X1[ii,:,0,0])\n","\t\tplt.plot(X1[ii,:,1,0])\n","\t\tplt.show()\n","\t\tprint(\"******************\")\n","\n","\treturn mygenerateddata"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"f8hUSPAmDlnB"},"outputs":[],"source":["mygenerateddata = generateNewSamples()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Itr4legrPzA"},"outputs":[],"source":["mygenerateddata_inv = mygenerateddata\n","mygenerateddata_inv[0][:,:,0,0] = scx0.inverse_transform(mygenerateddata[0][:,:,0,0])\n","mygenerateddata_inv[0][:,:,1,0] = scx1.inverse_transform(mygenerateddata[0][:,:,1,0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zwbrpB_wrPzA"},"outputs":[],"source":["import pickle\n","pickle.dump(mygenerateddata_inv,open(\"generated_inv01.pickle\",\"wb\"))\n","mygenerateddata_inv[0].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HxYrVpGyrPzA"},"outputs":[],"source":["plt.plot(mygenerateddata_inv[0][0:100,:,1,0])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"NHhLMMZC5uq8"},"outputs":[],"source":["def mergeDatasets():\n","    print(\"generateddata shape: \",mygenerateddata[0].shape)\n","    print(\"original data shape before merging: \",dataset[0].shape)\n","\n","    mygenerateddata[2] = mygenerateddata[2].squeeze()\n","\n","    list_data_base = [dataset,mygenerateddata]\n","    total_data = len(dataset[0]) + len(mygenerateddata[0])\n","\n","    ## Merging the Generated data set to the existing one\n","    newdataset = np.ndarray((total_data,dataset[0].shape[1],dataset[0].shape[2],dataset[0].shape[3]))\n","    newlabels  = np.ndarray((total_data,))\n","    newonehot  = np.ndarray((total_data,16))\n","\n","    newdataset[0:len(dataset[0])] = dataset[0]\n","    newdataset[len(dataset[0]):] = mygenerateddata[0]\n","\n","    newlabels[0:len(dataset[1])] = dataset[1]\n","    newlabels[len(dataset[1]):] = mygenerateddata[1]\n","\n","    newonehot[0:len(dataset[2])] = dataset[2]\n","    newonehot[len(dataset[2]):] = mygenerateddata[2]\n","\n","    print(\"dataset shape after merging: \",newdataset.shape)\n","    datasetNew = [newdataset,newlabels,newonehot]\n","    return datasetNew\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xDt8zwsudsmS"},"outputs":[],"source":["MergedDataset = mergeDatasets()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WK75yD0xrPzB"},"outputs":[],"source":["list_classLabels,counterTrue,counterFalse=postprocess(classifier,dataset)\n","list_classLabelsGen,counterTrue,counterFalse=postprocess(classifier,mygenerateddata)\n","list_classLabelsMerged,counterTrue,counterFalse=postprocess(classifier,MergedDataset)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mybLXMrlrPzB"},"outputs":[],"source":["print(\"Generated Classes after Conditional GAN training\")\n","plt.scatter(np.arange(len(list_classLabelsGen)),mygenerateddata[1])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"du7UpsQrrPzB"},"outputs":[],"source":["print(\"Generated Classes Determined by the Classifier Trained with Original Dataset\")\n","plt.scatter(np.arange(len(list_classLabelsGen)),list_classLabelsGen)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"mEubcd3yrPzC"},"source":["Sonuçlar incelendiğinde:\n","    - Her sınıftan eşit sayıda sınıf oluşturması gerekirken bunu yapmadığı,\n","    - Conditional GAN'ın 5, 7, ve 8. sınıflardan yeni sınıf oluşturmada zorlandığı,\n","    - oluşturulan sınıfların classifier tarafından sınıflandırıldığında, farklı label'lı sınıf olarak gördüğü\n","anlaşılmakta.\n","\n","Ancak, yine de kontrolsüz de olsa, birden fazla sınıf için yeni veri üretebilmiş gibiyiz. Bunu test etmek için knn vb yöntemleri de kullanarak sınıflandırma yapalım."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jz71h5crrPzJ"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hj6tWf7ndsmW"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"cdcgan_RF.ipynb","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"nbTranslate":{"displayLangs":["*"],"hotkey":"alt-t","langInMainMenu":true,"sourceLang":"en","targetLang":"fr","useGoogleTranslate":true},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":true}},"nbformat":4,"nbformat_minor":0}